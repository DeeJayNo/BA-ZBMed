{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model auswählen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mistralai/Mistral-7B-v0.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"BioMistral/BioMistral-7B\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import MistralForSequenceClassification, TrainingArguments, Trainer, BitsAndBytesConfig, DataCollatorWithPadding \n",
    "\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "import os\n",
    "# Definieren Sie das Cache-Verzeichnis\n",
    "cache_dir = '/media/ubuntu/5d2d9f9d-a02d-45ab-865f-3d789a0c70f0/download/'\n",
    "os.environ['TRANSFORMERS_CACHE'] = cache_dir\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### def Zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # HTML-Tags entfernen\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    text = re.sub(r\"[\\\",\\']\",\"\", text)  #  Anführungszeichen entfernen\n",
    "\n",
    "    # 1. Mehrfache Anführungszeichen durch ein normales ' ersetzen\n",
    "    text = re.sub(r\"'{2,}\", \"'\", text)\n",
    "\n",
    "    # 2. HTML-Tags entfernen [1, 2, 3]\n",
    "    # Sucht nach Mustern wie <tag>Inhalt</tag> und ersetzt sie durch einen leeren String.\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "\n",
    "    # 3. URLs entfernen [1, 2, 3]\n",
    "    # Sucht nach gängigen URL-Mustern (http/https, www.) und ersetzt sie durch einen leeren String.\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "    # 4. E-Mail-IDs entfernen [3]\n",
    "    # Sucht nach E-Mail-Mustern (Zeichenfolge@Zeichenfolge.Domain) und ersetzt sie durch einen leeren String.\n",
    "    text = re.sub(r'\\S*@\\S*\\s?', '', text)\n",
    "\n",
    "    # 5. Zusätzliche Leerzeichen normalisieren [1, 4]\n",
    "    # Teilt den Text nach Leerzeichen auf und fügt ihn mit einem einzigen Leerzeichen wieder zusammen.\n",
    "    text = \" \".join(text.split())\n",
    "\n",
    "    text = re.sub(r\"[\\[,\\]]\",\"\", text)  # Mehrfache Leerzeichen zu einem reduzieren\n",
    "    \n",
    "\n",
    "    return text\n",
    "\n",
    "# Hilfsfunktion, um die Reduzierung der trainierbaren Parameter zu sehen\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"Prints the number of trainable parameters in the model.\"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || \"\n",
    "        f\"trainable%: {100 * trainable_params / all_param:.2f}\"\n",
    "    )\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    preds = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    # Berechnung des gewichteten F1-Scores\n",
    "    f1 = f1_score(labels, preds, average='weighted')\n",
    "    \n",
    "    # Optional: Berechnung weiterer Metriken\n",
    "    precision, recall, _, _ = precision_recall_fscore_support(labels, preds, average='weighted', zero_division=0) # zero_division=0, um Warnungen zu vermeiden\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    \n",
    "    return {\n",
    "        'f1': f1,\n",
    "        'accuracy': acc,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=256) # max_length ggf. anpassen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Klassen erweitern und Daten aus Pandas DataFrame laden ---\n",
    "\n",
    "# Die Klassen wurden um \"Umweltwissenschaft\" und \"Rest\" erweitert.\n",
    "id2label = {0: \"Medizin\", 1: \"Ernährung\", 2: \"Landwirtschaft\", 3: \"Umweltwissenschaften\", 4: \"Rest\"}\n",
    "label2id = {key: value for value, key in id2label.items()}\n",
    "NUM_LABELS = len(id2label)\n",
    "\n",
    "# SIMULATION: Erstellen eines Beispiel-DataFrames.\n",
    "# Ersetzen Sie diesen Block durch das Laden Ihrer eigenen Daten, z.B.:\n",
    "# df = pd.read_csv('ihre_publikationen.csv')\n",
    "path_train='../01_Daten/pkl/df_all_15k-2.pkl'\n",
    "path_test='../01_Daten/pkl/df_val_5k-2.pkl'\n",
    "df = pd.read_pickle(path_train)\n",
    "df_test = pd.read_pickle(path_test)\n",
    "df['text'] = df['title'].astype(str) + \" - \" + df['abstract'].astype(str)\n",
    "df_test['text'] = df_test['title'].astype(str) + \" - \" + df_test['abstract'].astype(str)\n",
    "\n",
    "# text cleanen\n",
    "if \"cased\" in model_name:\n",
    "    print(\"ohne lower...\")\n",
    "    df[\"text\"] = df[\"text\"].apply(clean_text)\n",
    "    df_test['text_clean'] = df_test['text'].apply(clean_text)\n",
    "else:\n",
    "    df[\"text\"] = df[\"text\"].apply(clean_text).str.lower()\n",
    "    df_test['text_clean'] = df_test['text'].apply(clean_text).str.lower()\n",
    "\n",
    "\n",
    "df['text_clean'] = df['text'].apply(clean_text)\n",
    "df_test['text_clean'] = df_test['text'].apply(clean_text)\n",
    "\n",
    "# Umwandeln der Text-Labels (golden_record) in numerische IDs.\n",
    "df['class'] = df['class'].str.replace(r'ErnÃ¤hrung', 'Ernährung', regex=True)\n",
    "df_test['class'] = df_test['class'].str.replace(r'ErnÃ¤hrung', 'Ernährung', regex=True)\n",
    "df['label_enc'] = df['class'].map(label2id)\n",
    "df_test['label_enc'] = df_test['class'].map(label2id)\n",
    "\n",
    "df['label_enc'] = df['label_enc'].astype(int)\n",
    "df_test['label_enc'] = df_test['label_enc'].astype(int)\n",
    "\n",
    "\n",
    "# Erstellen eines Hugging Face Datasets aus dem Pandas DataFrame\n",
    "# Wir benötigen nur noch die Spalten 'text' und 'label'\n",
    "final_df = df[['text_clean', 'label_enc']]\n",
    "final_df_test = df_test[['text_clean', 'label_enc']]\n",
    "\n",
    "# umbenennen der Spalten \n",
    "final_df=final_df.rename(columns={\"label_enc\":\"labels\"})\n",
    "final_df=final_df.rename(columns={\"text_clean\":\"text\"})\n",
    "\n",
    "final_df_test=final_df_test.rename(columns={\"label_enc\":\"labels\"})\n",
    "final_df_test=final_df_test.rename(columns={\"text_clean\":\"text\"})\n",
    "\n",
    "dataset = Dataset.from_pandas(final_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modell Laden und mit QLoRA \"umhüllen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Modell und Tokenizer laden (mit 4-bit Quantisierung) ---\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "\n",
    "model = MistralForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=NUM_LABELS,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "print(\"\\nTrainierbare Parameter vor Anwendung von LoRA:\")\n",
    "print_trainable_parameters(model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "# 1. Modell für das k-bit Training vorbereiten\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# 2. LoRA-Konfiguration erstellen\n",
    "# Hier sagen wir PEFT, welche Schichten des Modells adaptiert werden sollen.\n",
    "# Für Mistral sind das typischerweise die Aufmerksamkeits-Schichten.\n",
    "lora_config = LoraConfig(\n",
    "    r=16,                           # Rank der LoRA-Matrizen (üblicher Wert: 8, 16, 32)\n",
    "    lora_alpha=32,                  # Alpha-Skalierungsfaktor\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\"             # Wichtig für Klassifizierungsaufgaben!\n",
    ")\n",
    "\n",
    "# 3. Das Basismodell mit der LoRA-Konfiguration \"umwickeln\"\n",
    "model = get_peft_model(model, lora_config)\n",
    "print(\"\\nBasismodell wird mit der LoRA-Konfiguration \\\"umwickelt\\\"\")\n",
    "\n",
    "print(\"\\nTrainierbare Parameter nach Anwendung von LoRA:\")\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokeniesierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6974e258432490ba8566d2fc0d62fa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- 3. Daten für das Training vorbereiten ---\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "#Aufteilung in Trainings- und Testset für eine robustere Evaluierung\n",
    "splits = tokenized_dataset.train_test_split(test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  trainieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starte das Fine-Tuning mit erweitertem Datensatz...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/ubuntu/5d2d9f9d-a02d-45ab-865f-3d789a0c70f0/BA/Novak/.venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='752' max='752' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [752/752 6:09:13, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.842040</td>\n",
       "      <td>0.686106</td>\n",
       "      <td>0.689333</td>\n",
       "      <td>0.704341</td>\n",
       "      <td>0.689333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.890496</td>\n",
       "      <td>0.652152</td>\n",
       "      <td>0.655667</td>\n",
       "      <td>0.702051</td>\n",
       "      <td>0.655667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.887447</td>\n",
       "      <td>0.709602</td>\n",
       "      <td>0.707667</td>\n",
       "      <td>0.713755</td>\n",
       "      <td>0.707667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.183387</td>\n",
       "      <td>0.685023</td>\n",
       "      <td>0.683333</td>\n",
       "      <td>0.697787</td>\n",
       "      <td>0.683333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.700010</td>\n",
       "      <td>0.683649</td>\n",
       "      <td>0.684000</td>\n",
       "      <td>0.691688</td>\n",
       "      <td>0.684000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.742400</td>\n",
       "      <td>2.146274</td>\n",
       "      <td>0.692248</td>\n",
       "      <td>0.692333</td>\n",
       "      <td>0.694559</td>\n",
       "      <td>0.692333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.742400</td>\n",
       "      <td>2.746061</td>\n",
       "      <td>0.696087</td>\n",
       "      <td>0.697333</td>\n",
       "      <td>0.696978</td>\n",
       "      <td>0.697333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.742400</td>\n",
       "      <td>2.884153</td>\n",
       "      <td>0.695430</td>\n",
       "      <td>0.695333</td>\n",
       "      <td>0.698191</td>\n",
       "      <td>0.695333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/ubuntu/5d2d9f9d-a02d-45ab-865f-3d789a0c70f0/BA/Novak/.venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/media/ubuntu/5d2d9f9d-a02d-45ab-865f-3d789a0c70f0/BA/Novak/.venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/media/ubuntu/5d2d9f9d-a02d-45ab-865f-3d789a0c70f0/BA/Novak/.venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/media/ubuntu/5d2d9f9d-a02d-45ab-865f-3d789a0c70f0/BA/Novak/.venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/media/ubuntu/5d2d9f9d-a02d-45ab-865f-3d789a0c70f0/BA/Novak/.venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/media/ubuntu/5d2d9f9d-a02d-45ab-865f-3d789a0c70f0/BA/Novak/.venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/media/ubuntu/5d2d9f9d-a02d-45ab-865f-3d789a0c70f0/BA/Novak/.venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training abgeschlossen!\n",
      "Finales Modell wurde unter ./mistral_classifier_final_v2 gespeichert.\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Training durchführen ---\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"./mistral_classifier_results_v2/{model_name}\",\n",
    "    num_train_epochs=8,\n",
    "    per_device_train_batch_size=32,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    logging_dir='./logs_lora/{model_name}',\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=splits[\"train\"],\n",
    "    eval_dataset=splits[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"\\nStarte das Fine-Tuning mit erweitertem Datensatz...\")\n",
    "trainer.train()\n",
    "print(\"Training abgeschlossen!\")\n",
    "\n",
    "final_model_path = f\"./mistral_classifier_final_v2/{model_name}\"\n",
    "trainer.save_model(final_model_path)\n",
    "tokenizer.save_pretrained(final_model_path)\n",
    "lora_config.save_pretrained(final_model_path)\n",
    "\n",
    "print(f\"Finales Modell wurde unter {final_model_path} gespeichert.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
