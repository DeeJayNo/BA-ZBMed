{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ded01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 12 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07e130b5a0a94599bbb1925156d84fa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=1250), Label(value='0 / 1250'))), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing training data...\n",
      "End...\n",
      "creating dataset...\n",
      "End...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afdbb8ae5bd14a3b914510ccf5d34629",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=417), Label(value='0 / 417'))), HB…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing validation data...\n",
      "End...\n",
      "creating dataset...\n",
      "End...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_370305/3431643262.py:305: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[I 2025-06-02 16:05:54,812] A new study created in memory with name: no-name-b13a94b3-e707-4b1e-a7ee-8285a6f2f7f8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starte Hyperparameter-Suche...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1875' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1875/1875 31:16, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.792600</td>\n",
       "      <td>0.825707</td>\n",
       "      <td>0.706056</td>\n",
       "      <td>0.706667</td>\n",
       "      <td>0.707592</td>\n",
       "      <td>0.706667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.687400</td>\n",
       "      <td>0.786757</td>\n",
       "      <td>0.708340</td>\n",
       "      <td>0.708667</td>\n",
       "      <td>0.708837</td>\n",
       "      <td>0.708667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.627500</td>\n",
       "      <td>0.801096</td>\n",
       "      <td>0.719225</td>\n",
       "      <td>0.721000</td>\n",
       "      <td>0.724871</td>\n",
       "      <td>0.721000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.545300</td>\n",
       "      <td>0.801447</td>\n",
       "      <td>0.722051</td>\n",
       "      <td>0.723667</td>\n",
       "      <td>0.724230</td>\n",
       "      <td>0.723667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.428400</td>\n",
       "      <td>0.815464</td>\n",
       "      <td>0.720134</td>\n",
       "      <td>0.722333</td>\n",
       "      <td>0.723226</td>\n",
       "      <td>0.722333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-02 16:37:15,368] Trial 0 finished with value: 2.8880271159143724 and parameters: {'learning_rate': 1.3630463847311672e-05, 'num_train_epochs': 5, 'per_device_train_batch_size': 32, 'weight_decay': 0.0}. Best is trial 0 with value: 2.8880271159143724.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9000' max='9000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9000/9000 1:18:32, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.664800</td>\n",
       "      <td>0.812175</td>\n",
       "      <td>0.701731</td>\n",
       "      <td>0.702333</td>\n",
       "      <td>0.710369</td>\n",
       "      <td>0.702333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.602400</td>\n",
       "      <td>0.783353</td>\n",
       "      <td>0.716705</td>\n",
       "      <td>0.719333</td>\n",
       "      <td>0.718818</td>\n",
       "      <td>0.719333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.487000</td>\n",
       "      <td>0.838251</td>\n",
       "      <td>0.708107</td>\n",
       "      <td>0.709333</td>\n",
       "      <td>0.713508</td>\n",
       "      <td>0.709333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.306200</td>\n",
       "      <td>0.949674</td>\n",
       "      <td>0.716921</td>\n",
       "      <td>0.718667</td>\n",
       "      <td>0.716768</td>\n",
       "      <td>0.718667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.230500</td>\n",
       "      <td>1.176032</td>\n",
       "      <td>0.714350</td>\n",
       "      <td>0.717000</td>\n",
       "      <td>0.715378</td>\n",
       "      <td>0.717000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.120500</td>\n",
       "      <td>1.422783</td>\n",
       "      <td>0.709016</td>\n",
       "      <td>0.709667</td>\n",
       "      <td>0.713284</td>\n",
       "      <td>0.709667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.061300</td>\n",
       "      <td>1.633317</td>\n",
       "      <td>0.708847</td>\n",
       "      <td>0.711000</td>\n",
       "      <td>0.710289</td>\n",
       "      <td>0.711000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.072200</td>\n",
       "      <td>1.898704</td>\n",
       "      <td>0.704817</td>\n",
       "      <td>0.706333</td>\n",
       "      <td>0.708368</td>\n",
       "      <td>0.706333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.072300</td>\n",
       "      <td>2.034156</td>\n",
       "      <td>0.713392</td>\n",
       "      <td>0.714333</td>\n",
       "      <td>0.712989</td>\n",
       "      <td>0.714333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.032400</td>\n",
       "      <td>2.120025</td>\n",
       "      <td>0.709765</td>\n",
       "      <td>0.710667</td>\n",
       "      <td>0.710216</td>\n",
       "      <td>0.710667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.011200</td>\n",
       "      <td>2.154927</td>\n",
       "      <td>0.713866</td>\n",
       "      <td>0.715667</td>\n",
       "      <td>0.713158</td>\n",
       "      <td>0.715667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.012300</td>\n",
       "      <td>2.180497</td>\n",
       "      <td>0.713504</td>\n",
       "      <td>0.715333</td>\n",
       "      <td>0.713960</td>\n",
       "      <td>0.715333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-02 17:55:51,283] Trial 1 finished with value: 2.8581302059047884 and parameters: {'learning_rate': 1.7661512995208496e-05, 'num_train_epochs': 12, 'per_device_train_batch_size': 16, 'weight_decay': 0.01}. Best is trial 0 with value: 2.8880271159143724.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6000' max='6000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6000/6000 1:40:11, Epoch 16/16]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.765900</td>\n",
       "      <td>0.823753</td>\n",
       "      <td>0.701472</td>\n",
       "      <td>0.702667</td>\n",
       "      <td>0.707855</td>\n",
       "      <td>0.702667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.651600</td>\n",
       "      <td>0.788330</td>\n",
       "      <td>0.716503</td>\n",
       "      <td>0.717000</td>\n",
       "      <td>0.719380</td>\n",
       "      <td>0.717000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.526800</td>\n",
       "      <td>0.814014</td>\n",
       "      <td>0.718211</td>\n",
       "      <td>0.718333</td>\n",
       "      <td>0.722438</td>\n",
       "      <td>0.718333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.938873</td>\n",
       "      <td>0.711785</td>\n",
       "      <td>0.712333</td>\n",
       "      <td>0.714687</td>\n",
       "      <td>0.712333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.181100</td>\n",
       "      <td>1.124742</td>\n",
       "      <td>0.711000</td>\n",
       "      <td>0.710667</td>\n",
       "      <td>0.715560</td>\n",
       "      <td>0.710667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.152400</td>\n",
       "      <td>1.344617</td>\n",
       "      <td>0.706179</td>\n",
       "      <td>0.706333</td>\n",
       "      <td>0.712351</td>\n",
       "      <td>0.706333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.088200</td>\n",
       "      <td>1.559387</td>\n",
       "      <td>0.702861</td>\n",
       "      <td>0.703667</td>\n",
       "      <td>0.705743</td>\n",
       "      <td>0.703667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.052500</td>\n",
       "      <td>1.790332</td>\n",
       "      <td>0.709161</td>\n",
       "      <td>0.710000</td>\n",
       "      <td>0.710434</td>\n",
       "      <td>0.710000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.023300</td>\n",
       "      <td>1.956780</td>\n",
       "      <td>0.702232</td>\n",
       "      <td>0.704667</td>\n",
       "      <td>0.704010</td>\n",
       "      <td>0.704667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.038200</td>\n",
       "      <td>2.064751</td>\n",
       "      <td>0.704156</td>\n",
       "      <td>0.705000</td>\n",
       "      <td>0.706834</td>\n",
       "      <td>0.705000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.017800</td>\n",
       "      <td>2.166824</td>\n",
       "      <td>0.705797</td>\n",
       "      <td>0.707333</td>\n",
       "      <td>0.707063</td>\n",
       "      <td>0.707333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.008600</td>\n",
       "      <td>2.182248</td>\n",
       "      <td>0.705015</td>\n",
       "      <td>0.706667</td>\n",
       "      <td>0.705115</td>\n",
       "      <td>0.706667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>2.255381</td>\n",
       "      <td>0.710727</td>\n",
       "      <td>0.711333</td>\n",
       "      <td>0.712441</td>\n",
       "      <td>0.711333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.017500</td>\n",
       "      <td>2.281007</td>\n",
       "      <td>0.708458</td>\n",
       "      <td>0.711000</td>\n",
       "      <td>0.709706</td>\n",
       "      <td>0.711000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>2.302874</td>\n",
       "      <td>0.705484</td>\n",
       "      <td>0.706333</td>\n",
       "      <td>0.707143</td>\n",
       "      <td>0.706333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>2.310141</td>\n",
       "      <td>0.709763</td>\n",
       "      <td>0.711667</td>\n",
       "      <td>0.710793</td>\n",
       "      <td>0.711667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-02 19:36:07,388] Trial 2 finished with value: 2.8438892510453604 and parameters: {'learning_rate': 2.6513260375521086e-05, 'num_train_epochs': 16, 'per_device_train_batch_size': 32, 'weight_decay': 0.0}. Best is trial 0 with value: 2.8880271159143724.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4500' max='4500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4500/4500 1:15:08, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.761100</td>\n",
       "      <td>0.824544</td>\n",
       "      <td>0.694704</td>\n",
       "      <td>0.695000</td>\n",
       "      <td>0.700367</td>\n",
       "      <td>0.695000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.651500</td>\n",
       "      <td>0.780682</td>\n",
       "      <td>0.714830</td>\n",
       "      <td>0.714667</td>\n",
       "      <td>0.717338</td>\n",
       "      <td>0.714667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.560900</td>\n",
       "      <td>0.825522</td>\n",
       "      <td>0.713032</td>\n",
       "      <td>0.713667</td>\n",
       "      <td>0.719793</td>\n",
       "      <td>0.713667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.397700</td>\n",
       "      <td>0.890996</td>\n",
       "      <td>0.716396</td>\n",
       "      <td>0.717333</td>\n",
       "      <td>0.718145</td>\n",
       "      <td>0.717333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.228600</td>\n",
       "      <td>1.033077</td>\n",
       "      <td>0.717087</td>\n",
       "      <td>0.718667</td>\n",
       "      <td>0.718290</td>\n",
       "      <td>0.718667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.126300</td>\n",
       "      <td>1.205544</td>\n",
       "      <td>0.708557</td>\n",
       "      <td>0.709000</td>\n",
       "      <td>0.709178</td>\n",
       "      <td>0.709000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.103600</td>\n",
       "      <td>1.350276</td>\n",
       "      <td>0.711611</td>\n",
       "      <td>0.712667</td>\n",
       "      <td>0.711766</td>\n",
       "      <td>0.712667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.025200</td>\n",
       "      <td>1.528785</td>\n",
       "      <td>0.711408</td>\n",
       "      <td>0.713000</td>\n",
       "      <td>0.712312</td>\n",
       "      <td>0.713000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.019400</td>\n",
       "      <td>1.666014</td>\n",
       "      <td>0.708231</td>\n",
       "      <td>0.710333</td>\n",
       "      <td>0.708954</td>\n",
       "      <td>0.710333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.011000</td>\n",
       "      <td>1.774798</td>\n",
       "      <td>0.708945</td>\n",
       "      <td>0.713000</td>\n",
       "      <td>0.710951</td>\n",
       "      <td>0.713000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.014100</td>\n",
       "      <td>1.827976</td>\n",
       "      <td>0.715890</td>\n",
       "      <td>0.717000</td>\n",
       "      <td>0.716361</td>\n",
       "      <td>0.717000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.012700</td>\n",
       "      <td>1.854745</td>\n",
       "      <td>0.711012</td>\n",
       "      <td>0.712333</td>\n",
       "      <td>0.712083</td>\n",
       "      <td>0.712333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-02 20:51:20,442] Trial 3 finished with value: 2.8477608060780133 and parameters: {'learning_rate': 2.2768068181709884e-05, 'num_train_epochs': 12, 'per_device_train_batch_size': 32, 'weight_decay': 0.0}. Best is trial 0 with value: 2.8880271159143724.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12000' max='12000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12000/12000 1:44:34, Epoch 16/16]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.683700</td>\n",
       "      <td>0.827207</td>\n",
       "      <td>0.700404</td>\n",
       "      <td>0.701333</td>\n",
       "      <td>0.704176</td>\n",
       "      <td>0.701333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.697700</td>\n",
       "      <td>0.804993</td>\n",
       "      <td>0.705985</td>\n",
       "      <td>0.708000</td>\n",
       "      <td>0.707785</td>\n",
       "      <td>0.708000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.611400</td>\n",
       "      <td>0.793334</td>\n",
       "      <td>0.714558</td>\n",
       "      <td>0.716333</td>\n",
       "      <td>0.719386</td>\n",
       "      <td>0.716333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.546700</td>\n",
       "      <td>0.813843</td>\n",
       "      <td>0.712485</td>\n",
       "      <td>0.715000</td>\n",
       "      <td>0.717672</td>\n",
       "      <td>0.715000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.445800</td>\n",
       "      <td>0.841573</td>\n",
       "      <td>0.724291</td>\n",
       "      <td>0.727333</td>\n",
       "      <td>0.726477</td>\n",
       "      <td>0.727333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.430900</td>\n",
       "      <td>0.879890</td>\n",
       "      <td>0.722026</td>\n",
       "      <td>0.722667</td>\n",
       "      <td>0.723809</td>\n",
       "      <td>0.722667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.378500</td>\n",
       "      <td>0.940820</td>\n",
       "      <td>0.713698</td>\n",
       "      <td>0.716667</td>\n",
       "      <td>0.716554</td>\n",
       "      <td>0.716667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.214600</td>\n",
       "      <td>1.028816</td>\n",
       "      <td>0.709235</td>\n",
       "      <td>0.712333</td>\n",
       "      <td>0.712578</td>\n",
       "      <td>0.712333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.262100</td>\n",
       "      <td>1.085823</td>\n",
       "      <td>0.711754</td>\n",
       "      <td>0.716000</td>\n",
       "      <td>0.713032</td>\n",
       "      <td>0.716000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.272500</td>\n",
       "      <td>1.159829</td>\n",
       "      <td>0.713580</td>\n",
       "      <td>0.714667</td>\n",
       "      <td>0.713110</td>\n",
       "      <td>0.714667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.237700</td>\n",
       "      <td>1.234333</td>\n",
       "      <td>0.712686</td>\n",
       "      <td>0.714000</td>\n",
       "      <td>0.713270</td>\n",
       "      <td>0.714000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.083700</td>\n",
       "      <td>1.344885</td>\n",
       "      <td>0.703180</td>\n",
       "      <td>0.706333</td>\n",
       "      <td>0.708409</td>\n",
       "      <td>0.706333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.048200</td>\n",
       "      <td>1.395950</td>\n",
       "      <td>0.706324</td>\n",
       "      <td>0.709333</td>\n",
       "      <td>0.709518</td>\n",
       "      <td>0.709333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.056700</td>\n",
       "      <td>1.465605</td>\n",
       "      <td>0.705384</td>\n",
       "      <td>0.708667</td>\n",
       "      <td>0.708199</td>\n",
       "      <td>0.708667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.054000</td>\n",
       "      <td>1.505413</td>\n",
       "      <td>0.703666</td>\n",
       "      <td>0.707000</td>\n",
       "      <td>0.706674</td>\n",
       "      <td>0.707000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.025900</td>\n",
       "      <td>1.524677</td>\n",
       "      <td>0.703366</td>\n",
       "      <td>0.707000</td>\n",
       "      <td>0.707673</td>\n",
       "      <td>0.707000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-02 22:35:59,018] Trial 4 finished with value: 2.8250390048728593 and parameters: {'learning_rate': 6.589528693399998e-06, 'num_train_epochs': 16, 'per_device_train_batch_size': 16, 'weight_decay': 0.0}. Best is trial 0 with value: 2.8880271159143724.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='3750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 750/3750 06:29 < 26:02, 1.92 it/s, Epoch 1/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.891200</td>\n",
       "      <td>0.968751</td>\n",
       "      <td>0.656946</td>\n",
       "      <td>0.659333</td>\n",
       "      <td>0.660087</td>\n",
       "      <td>0.659333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-02 22:42:30,795] Trial 5 pruned. \n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='6000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 375/6000 06:12 < 1:33:43, 1.00 it/s, Epoch 1/16]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.932900</td>\n",
       "      <td>0.928604</td>\n",
       "      <td>0.664927</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.668010</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-02 22:48:46,461] Trial 6 pruned. \n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1500' max='12000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1500/12000 13:03 < 1:31:31, 1.91 it/s, Epoch 2/16]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.687000</td>\n",
       "      <td>0.806756</td>\n",
       "      <td>0.706949</td>\n",
       "      <td>0.707667</td>\n",
       "      <td>0.713612</td>\n",
       "      <td>0.707667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.635900</td>\n",
       "      <td>0.789152</td>\n",
       "      <td>0.714553</td>\n",
       "      <td>0.717667</td>\n",
       "      <td>0.717924</td>\n",
       "      <td>0.717667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-02 23:01:52,417] Trial 7 pruned. \n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='6000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 375/6000 06:12 < 1:33:34, 1.00 it/s, Epoch 1/16]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.750600</td>\n",
       "      <td>0.823668</td>\n",
       "      <td>0.692736</td>\n",
       "      <td>0.693333</td>\n",
       "      <td>0.698858</td>\n",
       "      <td>0.693333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-02 23:08:07,394] Trial 8 pruned. \n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='12000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  750/12000 06:29 < 1:37:44, 1.92 it/s, Epoch 1/16]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.734500</td>\n",
       "      <td>0.846921</td>\n",
       "      <td>0.690411</td>\n",
       "      <td>0.691000</td>\n",
       "      <td>0.692388</td>\n",
       "      <td>0.691000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-02 23:14:39,445] Trial 9 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Hyperparameter-Suche abgeschlossen ---\n",
      "Beste Trial:\n",
      "\n",
      "------------------------------------------\n",
      "learning_rate: 1.3630463847311672e-05\n",
      "num_train_epochs: 5\n",
      "per_device_train_batch_size : 32\n",
      "weight_decay: 0.0\n",
      "\n",
      "------------------END---------------------\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training the final model with the best hyperparameters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1875' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1875/1875 32:53, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.805660</td>\n",
       "      <td>0.694106</td>\n",
       "      <td>0.694200</td>\n",
       "      <td>0.697607</td>\n",
       "      <td>0.694200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.913500</td>\n",
       "      <td>0.763778</td>\n",
       "      <td>0.718022</td>\n",
       "      <td>0.718400</td>\n",
       "      <td>0.719416</td>\n",
       "      <td>0.718400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.660100</td>\n",
       "      <td>0.777008</td>\n",
       "      <td>0.717285</td>\n",
       "      <td>0.718800</td>\n",
       "      <td>0.726233</td>\n",
       "      <td>0.718800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.546700</td>\n",
       "      <td>0.781493</td>\n",
       "      <td>0.718497</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.723845</td>\n",
       "      <td>0.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.546700</td>\n",
       "      <td>0.793662</td>\n",
       "      <td>0.717350</td>\n",
       "      <td>0.719600</td>\n",
       "      <td>0.724653</td>\n",
       "      <td>0.719600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating the final model on the test set...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Set Evaluation Results:\n",
      "  eval_loss: 0.8004\n",
      "  eval_f1: 0.7209\n",
      "  eval_accuracy: 0.7223\n",
      "  eval_precision: 0.7235\n",
      "  eval_recall: 0.7223\n",
      "  eval_runtime: 29.3764\n",
      "  eval_samples_per_second: 102.1230\n",
      "  eval_steps_per_second: 12.7650\n",
      "  epoch: 5.0000\n",
      "\n",
      "Evaluating the final model on the validation set...\n",
      "\n",
      "Val Set Evaluation Results:\n",
      "  eval_loss: 0.7815\n",
      "  eval_f1: 0.7185\n",
      "  eval_accuracy: 0.7200\n",
      "  eval_precision: 0.7238\n",
      "  eval_recall: 0.7200\n",
      "  eval_runtime: 49.8475\n",
      "  eval_samples_per_second: 100.3060\n",
      "  eval_steps_per_second: 12.5380\n",
      "  epoch: 5.0000\n",
      "\n",
      " Saving the fine-tuned model and tokenizer...\n",
      "\n",
      "create predictions, to save in a df...\n",
      "--- 1. DF rdy ---\n",
      "Erstelle einen DataFrame nur mit den Vorhersagen, die vom Original abweichen (Validierungsset)...\n",
      "\n",
      "---  DFs rdy !!! ---\n",
      "--------------------------------------\n",
      "\n",
      "Anzahl der unterschiedlichen Vorhersagen im Validierungsset: 1400\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      " validation-set metrics (calculated from dfResults_pred):\n",
      "  Gewichteter F1-Score: 0.7185\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "Gesammelte Ergebnisse der Hyperparameter-Suche:\n",
      "    learning_rate  num_train_epochs  per_device_train_batch_size  \\\n",
      "0        0.000014                 5                           32   \n",
      "1        0.000014                 5                           32   \n",
      "2        0.000014                 5                           32   \n",
      "3        0.000014                 5                           32   \n",
      "4        0.000014                 5                           32   \n",
      "..            ...               ...                          ...   \n",
      "62       0.000004                16                           32   \n",
      "63       0.000020                16                           16   \n",
      "64       0.000020                16                           16   \n",
      "65       0.000025                16                           32   \n",
      "66       0.000004                16                           16   \n",
      "\n",
      "    weight_decay eval_dataset_type  eval_loss  eval_accuracy   eval_f1  \\\n",
      "0           0.00          train/HP   0.825707       0.706667  0.706056   \n",
      "1           0.00          train/HP   0.786757       0.708667  0.708340   \n",
      "2           0.00          train/HP   0.801096       0.721000  0.719225   \n",
      "3           0.00          train/HP   0.801447       0.723667  0.722051   \n",
      "4           0.00          train/HP   0.815464       0.722333  0.720134   \n",
      "..           ...               ...        ...            ...       ...   \n",
      "62          0.01          train/HP   0.928604       0.666667  0.664927   \n",
      "63          0.01          train/HP   0.806756       0.707667  0.706949   \n",
      "64          0.01          train/HP   0.789152       0.717667  0.714553   \n",
      "65          0.00          train/HP   0.823668       0.693333  0.692736   \n",
      "66          0.00          train/HP   0.846921       0.691000  0.690411   \n",
      "\n",
      "   eval_precision  eval_recall  \n",
      "0            None     0.706667  \n",
      "1            None     0.708667  \n",
      "2            None     0.721000  \n",
      "3            None     0.723667  \n",
      "4            None     0.722333  \n",
      "..            ...          ...  \n",
      "62           None     0.666667  \n",
      "63           None     0.707667  \n",
      "64           None     0.717667  \n",
      "65           None     0.693333  \n",
      "66           None     0.691000  \n",
      "\n",
      "[67 rows x 10 columns]\n",
      "runtimet: 464.66902928352357 min\n",
      "\n",
      " Script finished successfully!\n"
     ]
    }
   ],
   "source": [
    "boolHP = True # True: Hyperparameter-Suche, False: direktes Training mit festen Werten\n",
    "\n",
    "import time\n",
    "start_zeit = time.time()\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandarallel import pandarallel\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_recall_fscore_support\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer,  TrainingArguments, AutoConfig, AutoModelForSequenceClassification,DataCollatorWithPadding, TrainerCallback,TrainerState, TrainerControl\n",
    "import torch\n",
    "import os\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Initialisiere pandarallel für parallele Verarbeitung\n",
    "# Hier optional das Backend ändern, z.B. 'dask' oder 'ray' für größere Datensätze\n",
    "pandarallel.initialize(progress_bar=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Definieren Sie das Cache-Verzeichnis\n",
    "cache_dir = '/media/ubuntu/5d2d9f9d-a02d-45ab-865f-3d789a0c70f0/download/'\n",
    "os.environ['TRANSFORMERS_CACHE'] = cache_dir\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Dataset Klasse definieren\n",
    "class PublicationsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    # HTML-Tags entfernen\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    text = re.sub(r\"[\\\",\\']\",\"\", text)  #  Anführungszeichen entfernen\n",
    "\n",
    "    # 1. Mehrfache Anführungszeichen durch ein normales ' ersetzen\n",
    "    text = re.sub(r\"'{2,}\", \"'\", text)\n",
    "\n",
    "    # 2. HTML-Tags entfernen [1, 2, 3]\n",
    "    # Sucht nach Mustern wie <tag>Inhalt</tag> und ersetzt sie durch einen leeren String.\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "\n",
    "    # 3. URLs entfernen [1, 2, 3]\n",
    "    # Sucht nach gängigen URL-Mustern (http/https, www.) und ersetzt sie durch einen leeren String.\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "    # 4. E-Mail-IDs entfernen [3]\n",
    "    # Sucht nach E-Mail-Mustern (Zeichenfolge@Zeichenfolge.Domain) und ersetzt sie durch einen leeren String.\n",
    "    text = re.sub(r'\\S*@\\S*\\s?', '', text)\n",
    "\n",
    "    # 5. Zusätzliche Leerzeichen normalisieren [1, 4]\n",
    "    # Teilt den Text nach Leerzeichen auf und fügt ihn mit einem einzigen Leerzeichen wieder zusammen.\n",
    "    text = \" \".join(text.split())\n",
    "\n",
    "    text = re.sub(r\"[\\[,\\]]\",\"\", text)  # Mehrfache Leerzeichen zu einem reduzieren\n",
    "    \n",
    "\n",
    "    return text\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    preds = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    # Berechnung des gewichteten F1-Scores\n",
    "    f1 = f1_score(labels, preds, average='weighted')\n",
    "    \n",
    "    # Optional: Berechnung weiterer Metriken\n",
    "    precision, recall, _, _ = precision_recall_fscore_support(labels, preds, average='weighted', zero_division=0) # zero_division=0, um Warnungen zu vermeiden\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    \n",
    "    return {\n",
    "        'f1': f1,\n",
    "        'accuracy': acc,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "def model_init(trial):\n",
    "    # Laden Sie die Konfiguration zuerst, um sie an LoRaBertForSequenceClassification zu übergeben\n",
    "    # num_labels muss global oder als Argument verfügbar sein\n",
    "    \n",
    "    \n",
    "    #model_name = 'bert-base-uncased'\n",
    "    model_name = 'microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext'\n",
    "    config = AutoConfig.from_pretrained(model_name, num_labels=num_labels, cache_dir=cache_dir)\n",
    "\n",
    "    \n",
    "    return BertForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        config=config,\n",
    "        cache_dir=cache_dir\n",
    "    )\n",
    "\n",
    "def time_now():\n",
    "    # Zeit funktion für den Dateinamen\n",
    "    current_dateTime = datetime.now()\n",
    "    time = str(current_dateTime.hour+2)+\"-\"+str(current_dateTime.minute)+\"_\"+str(current_dateTime.day) +\"-\"+ str(current_dateTime.month)+\"-\"+str(current_dateTime.year)\n",
    "    return str(time)\n",
    "\n",
    "def hp_space_optuna(trial):\n",
    "    # Hyperparameter-Suchraum für Optuna\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 1e-4, log=True),\n",
    "        \"num_train_epochs\": trial.suggest_categorical(\"num_train_epochs\",  [5, 12, 16]),\n",
    "        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [16, 32]),\n",
    "        \"weight_decay\": trial.suggest_categorical(\"weight_decay\",  [0.0, 0.01]),\n",
    "    }\n",
    "\n",
    "\n",
    "def prepare_val(df):\n",
    "   \n",
    "    # Kombiniere Titel und Abstract\n",
    "    df['text'] = df['title'].astype(str) + \" - \" + df['abstract'].astype(str)\n",
    "\n",
    "    # Bereinigen Sie den Text\n",
    "    df[\"text\"] = df[\"text\"].parallel_apply(clean_text).str.lower()\n",
    "    # encode the labels\n",
    "    df['label_encoded'] = le.fit_transform(df['class']).astype(int)\n",
    "\n",
    "    df = df.sample(frac=1)\n",
    "    X_val = df[\"text\"]\n",
    "    Y_val = df[\"label_encoded\"]  \n",
    "\n",
    "    print(\"Tokenizing validation data...\")\n",
    "    val_encodings = tokenizer(\n",
    "        list(X_val), truncation=True, padding=True, max_length=512)\n",
    "    print(\"End...\")\n",
    "    print(\"creating dataset...\")\n",
    "    ## Dataset erstellen\n",
    "    val_dataset = PublicationsDataset(val_encodings, Y_val.reset_index(drop=True))\n",
    "    print(\"End...\")\n",
    "    return val_dataset,df\n",
    "\n",
    "def prepare_test_train(df):\n",
    "\n",
    "    df['text'] = df['title'].astype(str) + \" - \" + df['abstract'].astype(str)\n",
    "\n",
    "    # Bereinigen Sie den Text\n",
    "    df[\"text\"] = df[\"text\"].parallel_apply(clean_text).str.lower()\n",
    "\n",
    "    # encode the labels\n",
    "    df['label_encoded'] = le.fit_transform(df['class']).astype(int)\n",
    "\n",
    "    # train_test_split für das Training/Validation-Set (aus dfBert)\n",
    "    # Beachten Sie, dass X_test, y_test hier nur für das Training verwendet werden.\n",
    "    # dfBert_eval wird als separates Validierungsset für die Valedierung genutzt.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df['text'], df['label_encoded'], test_size=0.2, random_state=42, stratify=df['label_encoded'])\n",
    "\n",
    "    print(\"Tokenizing training data...\")\n",
    "    train_encodings = tokenizer(\n",
    "        list(X_train), truncation=True, padding=True, max_length=512)\n",
    "    test_encodings = tokenizer(\n",
    "        list(X_test), truncation=True, padding=True, max_length=512)\n",
    "    print(\"End...\")\n",
    "    print(\"creating dataset...\")\n",
    "    ## Dataset erstellen\n",
    "    train_dataset = PublicationsDataset(train_encodings, y_train.reset_index(drop=True))\n",
    "    test_dataset = PublicationsDataset(test_encodings, y_test.reset_index(drop=True))\n",
    "    print(\"End...\")\n",
    "    return train_dataset,test_dataset,df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- 1. Initialisierung ---\n",
    "\n",
    "hp_search_results_list = []\n",
    "final_run_results_list = []\n",
    "\n",
    "# # Definieren Sie den Pfad zu den Daten\n",
    "#path_train='../01_Daten/pkl/df_all_15k-1.pkl'\n",
    "path_train='../01_Daten/pkl/df_all_15k-2.pkl'\n",
    "#path_train='../01_Daten/pkl/df_all_15k-3.pkl'\n",
    "path_val='../01_Daten/pkl/df_val_5k-2.pkl'\n",
    "#path_val='../01_Daten/pkl/df_val_5k-3.pkl'\n",
    "\n",
    "\n",
    "#speicher Pfad für Logs und Modelle\n",
    "time_log_save = time_now()\n",
    "model_base_path = f\"../01_Daten/logs/{time_log_save}/PubMedBert_multiclass_FT-1k/\"\n",
    "model_log_path = model_base_path+\"logs/\"\n",
    "model_output_path = model_base_path+\"results/\"\n",
    "model_final_path = model_base_path+\"final_model/\"\n",
    "\n",
    "# LabelEncoder, tokenizer und  Data collator initialisieren\n",
    "le = LabelEncoder()\n",
    "#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', cache_dir=cache_dir)\n",
    "tokenizer = BertTokenizer.from_pretrained('microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext', cache_dir=cache_dir)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# --- 2. Erstellen HP Trainer und args---\n",
    "train_dataset, test_dataset, dfBert_train = prepare_test_train(pd.read_pickle(path_train))\n",
    "val_dataset, dfBert_val = prepare_val(pd.read_pickle(path_val)) # Das ist jetzt dein dediziertes Validierungsset\n",
    "\n",
    "if boolHP:\n",
    "\n",
    "    # num_labels auslesen für die Model-Initialisierung\n",
    "    num_labels = dfBert_train['label_encoded'].nunique()\n",
    "\n",
    "    # Trainingsparameter für die Hyperparameter-Suche, diese Werte dienen als Standardwerte oder Fallbacks.\n",
    "    # Die Werte aus Optuna (über hp_space_optuna) werden während der Trials verwendet.\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f'{model_output_path}results_hp_search', \n",
    "        \n",
    "        learning_rate=1e-5,\n",
    "        num_train_epochs=1, \n",
    "        per_device_train_batch_size= 16,        \n",
    "        \n",
    "        # Feste Werte für die Suche:\n",
    "        logging_dir=f'{model_log_path}logs_hp_search',\n",
    "        logging_steps=10,\n",
    "        report_to=\"tensorboard\",\n",
    "        eval_strategy=\"epoch\", \n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=1,\n",
    "        load_best_model_at_end=False,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        greater_is_better=True,\n",
    "    )\n",
    "    # Trainer initialisieren (ohne ein festes Modell - model_init wird verwendet)\n",
    "    trainer = Trainer(\n",
    "        model_init=model_init,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        #callbacks=[HPSearchResultLoggerCallback()], \n",
    "    )\n",
    "\n",
    "    # --- 3. Starten der Hyperparameter-Suche ---\n",
    "    print(\"Starte Hyperparameter-Suche...\")\n",
    "    # Starten der Hyperparameter-Suche\n",
    "    best_trial = trainer.hyperparameter_search(\n",
    "        direction=\"maximize\", # Maximiere den F1-Score\n",
    "        backend=\"optuna\",\n",
    "        n_trials=10, # Anzahl der Trials, die Optuna durchführen soll, je mehr Trials, desto länger dauert es, aber potenziell bessere Ergebnisse.\n",
    "        hp_space=hp_space_optuna,\n",
    "    )\n",
    "    print(\"\\n--- Hyperparameter-Suche abgeschlossen ---\")\n",
    "\n",
    "    print(\"Beste Trial:\")\n",
    "    print(f\"  Wert (F1): {best_trial}\") \n",
    "    print(\"\\n------------------------------------------\")\n",
    "    print(\"learning_rate: \"+str(best_trial.hyperparameters[\"learning_rate\"]))\n",
    "    print(\"num_train_epochs: \"+str(best_trial.hyperparameters[\"num_train_epochs\"]))\n",
    "    print(\"per_device_train_batch_size : \"+str(best_trial.hyperparameters[\"per_device_train_batch_size\"]))\n",
    "    #print(\"warmup_ratio: \"+str(best_trial.hyperparameters[\"warmup_ratio\"]))\n",
    "    print(\"weight_decay: \"+str(best_trial.hyperparameters[\"weight_decay\"]))\n",
    "    print(\"\\n------------------END---------------------\\n\\n\\n\")\n",
    "\n",
    "    # --- 4. Train with Best Hyperparameters ---\n",
    "    #update der TrainingArguments mit den besten Hyperparametern\n",
    "    best_hp = best_trial.hyperparameters\n",
    "\n",
    "\n",
    "    final_training_args = TrainingArguments(\n",
    "        output_dir=f\"{model_output_path}best_run\", \n",
    "        logging_dir=f\"{model_log_path}best_run\",\n",
    "        report_to=\"tensorboard\",\n",
    "        learning_rate=best_hp[\"learning_rate\"],\n",
    "        num_train_epochs=best_hp[\"num_train_epochs\"],\n",
    "        per_device_train_batch_size=best_hp[\"per_device_train_batch_size\"],\n",
    "        per_device_eval_batch_size=training_args.per_device_eval_batch_size, \n",
    "        weight_decay=best_hp.get(\"weight_decay\", training_args.weight_decay),\n",
    "        #warmup_ratio=best_hp.get(\"warmup_ratio\", training_args.warmup_ratio),\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=1, \n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        greater_is_better=True,\n",
    "        seed=42\n",
    "    )\n",
    "else:\n",
    "    final_training_args = TrainingArguments(\n",
    "    output_dir=f\"{model_output_path}best_run\", \n",
    "    logging_dir=f\"{model_log_path}best_run\",\n",
    "\n",
    "    learning_rate=1.3630463847311672e-05,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=64, \n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit = 1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    seed=42\n",
    "    )\n",
    "\n",
    "# Initialisiere den Trainer mit den statischen Hyperparametern\n",
    "final_trainer = Trainer(\n",
    "    model_init=model_init,\n",
    "    args=final_training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"\\nTraining the final model with the best hyperparameters...\")\n",
    "final_trainer.train()\n",
    "\n",
    "# --- 5. Evaluate the Final Model ---\n",
    "print(\"\\nEvaluating the final model on the test set...\")\n",
    "test_results = final_trainer.evaluate(test_dataset)\n",
    "print(\"\\nTest Set Evaluation Results:\")\n",
    "for key, value in test_results.items():\n",
    "    print(f\"  {key}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nEvaluating the final model on the validation set...\")\n",
    "val_results = final_trainer.evaluate(val_dataset)\n",
    "print(\"\\nVal Set Evaluation Results:\")\n",
    "for key, value in val_results.items():\n",
    "    print(f\"  {key}: {value:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if boolHP:\n",
    "    # # --- 6. Save the Final Model & Tokenizer ---\n",
    "    end_zeit = time.time()\n",
    "    laufzeit = end_zeit - start_zeit\n",
    "    print(\"\\n Saving the fine-tuned model and tokenizer...\")\n",
    "    final_trainer.save_model(f\"{model_final_path}model\")\n",
    "    tokenizer.save_pretrained(f\"{model_final_path}tokenizer/\")\n",
    "\n",
    "\n",
    "\n",
    "# # --- 7. Predict and to DF ---\n",
    "# --- Erstellen eines DataFrames mit Vorhersagen für das Validierungsset ---\n",
    "print(\"\\ncreate predictions, to save in a df...\")\n",
    "\n",
    "predictions_output_val = final_trainer.predict(val_dataset)\n",
    "predicted_scores_val = predictions_output_val.predictions\n",
    "predicted_labels_encoded_val = np.argmax(predicted_scores_val, axis=1)\n",
    "predicted_labels_named_val = le.inverse_transform(predicted_labels_encoded_val)\n",
    "\n",
    "dfResults_pred = pd.DataFrame()\n",
    "\n",
    "dfResults_pred['id_im_aktuellen_df'] = dfBert_val.index.values\n",
    "\n",
    "# Ursprüngliche Klasse (Text-Label) aus dfBert_val\n",
    "dfResults_pred['class_original'] = dfBert_val['class'].values\n",
    "\n",
    "# Ursprüngliche Klasse (numerisch kodiertes Label - Ground Truth) aus dfBert_val\n",
    "dfResults_pred['label_encoded_original'] = dfBert_val['label_encoded'].values\n",
    "\n",
    "# Vorhergesagte Klasse (numerisch kodiertes Label)\n",
    "dfResults_pred['prediction_encoded'] = predicted_labels_encoded_val\n",
    "\n",
    "# Vorhergesagte Klasse (Text-Label)\n",
    "dfResults_pred['prediction_named'] = predicted_labels_named_val\n",
    "\n",
    "# Optional: Fügen Sie den Text hinzu, der für die Vorhersage verwendet wurde\n",
    "dfResults_pred['text_input'] = dfBert_val['text'].values\n",
    "\n",
    "\n",
    "print(\"--- 1. DF rdy ---\")\n",
    "print(\"Erstelle einen DataFrame nur mit den Vorhersagen, die vom Original abweichen (Validierungsset)...\")\n",
    "# Filtere dfResults_pred, um nur Zeilen zu erhalten, bei denen das Original-Label und das vorhergesagte Label unterschiedlich sind.\n",
    "dfResults_pred_diff = dfResults_pred[dfResults_pred['label_encoded_original'] != dfResults_pred['prediction_encoded']]\n",
    "print(\"\\n---  DFs rdy !!! ---\")\n",
    "if dfResults_pred_diff.empty:\n",
    "    print(\"Keine Unterschiede zwischen Original- und Vorhersage-Labels im Validierungsset gefunden. Perfekte Vorhersage!\")\n",
    "else:\n",
    "    print(\"--------------------------------------\")\n",
    "    print(f\"\\nAnzahl der unterschiedlichen Vorhersagen im Validierungsset: {len(dfResults_pred_diff)}\")\n",
    "print(\"\\n-------------------------------------------\")\n",
    "\n",
    "# Wahre Labels und vorhergesagte Labels aus dem DataFrame extrahieren\n",
    "y_true_val = dfResults_pred['label_encoded_original']\n",
    "y_pred_val = dfResults_pred['prediction_encoded']\n",
    "\n",
    "print(\"\\n validation-set metrics (calculated from dfResults_pred):\")\n",
    "\n",
    "#Gewichteter F1-Score\n",
    "f1_val_weighted = f1_score(y_true_val, y_pred_val, average='weighted', zero_division=0)\n",
    "print(f\"  Gewichteter F1-Score: {f1_val_weighted:.4f}\")\n",
    "print(\"\\n------------------------------------------\")\n",
    "\n",
    "\n",
    "dfResults_pred.to_pickle(f\"{model_base_path}dfResults_pred.pkl\")\n",
    "dfResults_pred_diff.to_pickle(f\"{model_base_path}dfResults_pred_diff({len(dfResults_pred_diff)}).pkl\")\n",
    "\n",
    "\n",
    "print(f\"runtimet: {laufzeit/60} min\")\n",
    "print(\"\\n Script finished successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61167a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trial = trainer.hyperparameter_search(\n",
    "        direction=\"maximize\", # Maximiere den F1-Score\n",
    "        backend=\"optuna\",\n",
    "        n_trials=10, # Anzahl der Trials, die Optuna durchführen soll, je mehr Trials, desto länger dauert es, aber potenziell bessere Ergebnisse.\n",
    "        hp_space=hp_space_optuna,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e031bbad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "final_training_args2 = TrainingArguments(\n",
    "    output_dir=f\"{model_output_path}best_run2\", \n",
    "    logging_dir=f\"{model_log_path}best_run2\",\n",
    "    report_to=\"tensorboard\",\n",
    "    learning_rate=best_hp[\"learning_rate\"],\n",
    "    num_train_epochs=best_hp[\"num_train_epochs\"],\n",
    "    per_device_train_batch_size=best_hp[\"per_device_train_batch_size\"],\n",
    "    per_device_eval_batch_size=training_args.per_device_eval_batch_size, \n",
    "    weight_decay=best_hp.get(\"weight_decay\", training_args.weight_decay),\n",
    "    #warmup_ratio=best_hp.get(\"warmup_ratio\", training_args.warmup_ratio),\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1, \n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Initialisiere den Trainer mit den statischen Hyperparametern\n",
    "final_trainer2 = Trainer(\n",
    "    model_init=model_init,\n",
    "    args=final_training_args2,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00eea5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"------------------------------------------\")\n",
    "print(\"Beste Trial\\n:\")\n",
    "\n",
    "print(\"learning_rate: \"+str(best_trial.hyperparameters[\"learning_rate\"]))\n",
    "print(\"num_train_epochs: \"+str(best_trial.hyperparameters[\"num_train_epochs\"]))\n",
    "print(\"per_device_train_batch_size : \"+str(best_trial.hyperparameters[\"per_device_train_batch_size\"]))\n",
    "print(\"weight_decay: \"+str(best_trial.hyperparameters[\"weight_decay\"]))\n",
    "#print(\"test: \"+str(best_trial.hyperparameters[\"warmup_ratio\"]))\n",
    "\n",
    "print(\"\\n------------------END---------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c73f6523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training the final model with the best hyperparameters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1875' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1875/1875 32:55, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.805916</td>\n",
       "      <td>0.697725</td>\n",
       "      <td>0.697800</td>\n",
       "      <td>0.701484</td>\n",
       "      <td>0.697800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.911500</td>\n",
       "      <td>0.764255</td>\n",
       "      <td>0.717997</td>\n",
       "      <td>0.718200</td>\n",
       "      <td>0.718914</td>\n",
       "      <td>0.718200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.661100</td>\n",
       "      <td>0.779274</td>\n",
       "      <td>0.716233</td>\n",
       "      <td>0.717600</td>\n",
       "      <td>0.724006</td>\n",
       "      <td>0.717600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.546000</td>\n",
       "      <td>0.781090</td>\n",
       "      <td>0.717589</td>\n",
       "      <td>0.719200</td>\n",
       "      <td>0.722237</td>\n",
       "      <td>0.719200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.546000</td>\n",
       "      <td>0.792449</td>\n",
       "      <td>0.718652</td>\n",
       "      <td>0.720800</td>\n",
       "      <td>0.724259</td>\n",
       "      <td>0.720800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating the final model on the test set...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 01:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Set Evaluation Results:\n",
      "  eval_loss: 0.8094\n",
      "  eval_f1: 0.7260\n",
      "  eval_accuracy: 0.7280\n",
      "  eval_precision: 0.7282\n",
      "  eval_recall: 0.7280\n",
      "  eval_runtime: 29.5913\n",
      "  eval_samples_per_second: 101.3810\n",
      "  eval_steps_per_second: 12.6730\n",
      "  epoch: 5.0000\n",
      "\n",
      "Evaluating the final model on the validation set...\n",
      "\n",
      "Val Set Evaluation Results:\n",
      "  eval_loss: 0.7924\n",
      "  eval_f1: 0.7187\n",
      "  eval_accuracy: 0.7208\n",
      "  eval_precision: 0.7243\n",
      "  eval_recall: 0.7208\n",
      "  eval_runtime: 50.2634\n",
      "  eval_samples_per_second: 99.4760\n",
      "  eval_steps_per_second: 12.4340\n",
      "  epoch: 5.0000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"\\nTraining the final model with the best hyperparameters...\")\n",
    "final_trainer.train()\n",
    "\n",
    "# --- 5. Evaluate the Final Model ---\n",
    "print(\"\\nEvaluating the final model on the test set...\")\n",
    "test_results = final_trainer.evaluate(test_dataset)\n",
    "print(\"\\nTest Set Evaluation Results:\")\n",
    "for key, value in test_results.items():\n",
    "    print(f\"  {key}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nEvaluating the final model on the validation set...\")\n",
    "val_results = final_trainer.evaluate(val_dataset)\n",
    "print(\"\\nVal Set Evaluation Results:\")\n",
    "for key, value in val_results.items():\n",
    "    print(f\"  {key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b60bcfc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
