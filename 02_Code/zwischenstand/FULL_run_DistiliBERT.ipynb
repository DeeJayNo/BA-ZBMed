{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df707d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 12 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36e672fc9028467aa708d224732db9e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=1250), Label(value='0 / 1250'))), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing training data...\n",
      "End...\n",
      "creating dataset...\n",
      "End...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3838a22f15ef47ae882920b74cb33304",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=417), Label(value='0 / 417'))), HB…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing validation data...\n",
      "End...\n",
      "creating dataset...\n",
      "End...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[I 2025-06-09 17:24:07,934] A new study created in memory with name: no-name-43ebf83a-9e62-43cb-8bf4-701b83dc1a0b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starte Hyperparameter-Suche...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4500' max='4500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4500/4500 39:13, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.042400</td>\n",
       "      <td>1.013925</td>\n",
       "      <td>0.633483</td>\n",
       "      <td>0.638000</td>\n",
       "      <td>0.638485</td>\n",
       "      <td>0.638000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.880500</td>\n",
       "      <td>0.905621</td>\n",
       "      <td>0.662723</td>\n",
       "      <td>0.665333</td>\n",
       "      <td>0.674799</td>\n",
       "      <td>0.665333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.791200</td>\n",
       "      <td>0.850163</td>\n",
       "      <td>0.683810</td>\n",
       "      <td>0.686667</td>\n",
       "      <td>0.687112</td>\n",
       "      <td>0.686667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.797600</td>\n",
       "      <td>0.821861</td>\n",
       "      <td>0.698346</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.698103</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.746200</td>\n",
       "      <td>0.842817</td>\n",
       "      <td>0.682465</td>\n",
       "      <td>0.686333</td>\n",
       "      <td>0.692876</td>\n",
       "      <td>0.686333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.722500</td>\n",
       "      <td>0.810800</td>\n",
       "      <td>0.691850</td>\n",
       "      <td>0.694000</td>\n",
       "      <td>0.692766</td>\n",
       "      <td>0.694000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.686300</td>\n",
       "      <td>0.810430</td>\n",
       "      <td>0.695813</td>\n",
       "      <td>0.698333</td>\n",
       "      <td>0.697976</td>\n",
       "      <td>0.698333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.631100</td>\n",
       "      <td>0.817208</td>\n",
       "      <td>0.692705</td>\n",
       "      <td>0.696000</td>\n",
       "      <td>0.697199</td>\n",
       "      <td>0.696000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.591500</td>\n",
       "      <td>0.807862</td>\n",
       "      <td>0.696840</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.698164</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.620300</td>\n",
       "      <td>0.806497</td>\n",
       "      <td>0.699655</td>\n",
       "      <td>0.702333</td>\n",
       "      <td>0.700872</td>\n",
       "      <td>0.702333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.517600</td>\n",
       "      <td>0.809140</td>\n",
       "      <td>0.695947</td>\n",
       "      <td>0.698667</td>\n",
       "      <td>0.697828</td>\n",
       "      <td>0.698667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.635100</td>\n",
       "      <td>0.807986</td>\n",
       "      <td>0.697746</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.699750</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-09 18:03:26,684] Trial 0 finished with value: 2.797495603089045 and parameters: {'learning_rate': 3.2888095206990444e-06, 'num_train_epochs': 12, 'per_device_train_batch_size': 32, 'weight_decay': 0.01}. Best is trial 0 with value: 2.797495603089045.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1875' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1875/1875 16:20, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.781000</td>\n",
       "      <td>0.824273</td>\n",
       "      <td>0.682225</td>\n",
       "      <td>0.683667</td>\n",
       "      <td>0.697022</td>\n",
       "      <td>0.683667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.611800</td>\n",
       "      <td>0.771781</td>\n",
       "      <td>0.718011</td>\n",
       "      <td>0.720333</td>\n",
       "      <td>0.724284</td>\n",
       "      <td>0.720333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.363200</td>\n",
       "      <td>0.918350</td>\n",
       "      <td>0.716390</td>\n",
       "      <td>0.716667</td>\n",
       "      <td>0.722319</td>\n",
       "      <td>0.716667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.187300</td>\n",
       "      <td>1.091389</td>\n",
       "      <td>0.722485</td>\n",
       "      <td>0.722667</td>\n",
       "      <td>0.722950</td>\n",
       "      <td>0.722667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.053000</td>\n",
       "      <td>1.343204</td>\n",
       "      <td>0.722573</td>\n",
       "      <td>0.721667</td>\n",
       "      <td>0.723883</td>\n",
       "      <td>0.721667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-09 18:19:49,986] Trial 1 finished with value: 2.8897898528963553 and parameters: {'learning_rate': 9.16873665647161e-05, 'num_train_epochs': 5, 'per_device_train_batch_size': 32, 'weight_decay': 0.01}. Best is trial 1 with value: 2.8897898528963553.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9000' max='9000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9000/9000 41:41, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.743500</td>\n",
       "      <td>0.797811</td>\n",
       "      <td>0.703805</td>\n",
       "      <td>0.706667</td>\n",
       "      <td>0.711415</td>\n",
       "      <td>0.706667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.546300</td>\n",
       "      <td>0.839115</td>\n",
       "      <td>0.686878</td>\n",
       "      <td>0.690667</td>\n",
       "      <td>0.701639</td>\n",
       "      <td>0.690667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.368300</td>\n",
       "      <td>1.081276</td>\n",
       "      <td>0.676632</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.693364</td>\n",
       "      <td>0.680000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.365000</td>\n",
       "      <td>1.336544</td>\n",
       "      <td>0.696326</td>\n",
       "      <td>0.695000</td>\n",
       "      <td>0.708548</td>\n",
       "      <td>0.695000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.088200</td>\n",
       "      <td>1.678437</td>\n",
       "      <td>0.692031</td>\n",
       "      <td>0.692333</td>\n",
       "      <td>0.696870</td>\n",
       "      <td>0.692333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.119900</td>\n",
       "      <td>1.910957</td>\n",
       "      <td>0.698715</td>\n",
       "      <td>0.696000</td>\n",
       "      <td>0.707833</td>\n",
       "      <td>0.696000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.137200</td>\n",
       "      <td>2.124757</td>\n",
       "      <td>0.694544</td>\n",
       "      <td>0.693667</td>\n",
       "      <td>0.699220</td>\n",
       "      <td>0.693667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.029700</td>\n",
       "      <td>2.415919</td>\n",
       "      <td>0.708212</td>\n",
       "      <td>0.706000</td>\n",
       "      <td>0.713627</td>\n",
       "      <td>0.706000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.024100</td>\n",
       "      <td>2.463970</td>\n",
       "      <td>0.702251</td>\n",
       "      <td>0.701000</td>\n",
       "      <td>0.704115</td>\n",
       "      <td>0.701000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>2.534516</td>\n",
       "      <td>0.701981</td>\n",
       "      <td>0.702667</td>\n",
       "      <td>0.701674</td>\n",
       "      <td>0.702667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>2.696074</td>\n",
       "      <td>0.699265</td>\n",
       "      <td>0.698000</td>\n",
       "      <td>0.702479</td>\n",
       "      <td>0.698000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>2.718183</td>\n",
       "      <td>0.700218</td>\n",
       "      <td>0.698667</td>\n",
       "      <td>0.703938</td>\n",
       "      <td>0.698667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-09 19:01:33,944] Trial 2 finished with value: 2.801489046064531 and parameters: {'learning_rate': 8.598143793309295e-05, 'num_train_epochs': 12, 'per_device_train_batch_size': 16, 'weight_decay': 0.0}. Best is trial 1 with value: 2.8897898528963553.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4500' max='4500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4500/4500 39:13, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.788500</td>\n",
       "      <td>0.832422</td>\n",
       "      <td>0.684586</td>\n",
       "      <td>0.686333</td>\n",
       "      <td>0.696780</td>\n",
       "      <td>0.686333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.611300</td>\n",
       "      <td>0.786940</td>\n",
       "      <td>0.711927</td>\n",
       "      <td>0.716667</td>\n",
       "      <td>0.722232</td>\n",
       "      <td>0.716667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.383500</td>\n",
       "      <td>0.955733</td>\n",
       "      <td>0.707646</td>\n",
       "      <td>0.711000</td>\n",
       "      <td>0.710041</td>\n",
       "      <td>0.711000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.267500</td>\n",
       "      <td>1.051269</td>\n",
       "      <td>0.709430</td>\n",
       "      <td>0.706000</td>\n",
       "      <td>0.719543</td>\n",
       "      <td>0.706000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.118700</td>\n",
       "      <td>1.353183</td>\n",
       "      <td>0.698503</td>\n",
       "      <td>0.697000</td>\n",
       "      <td>0.705315</td>\n",
       "      <td>0.697000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.131300</td>\n",
       "      <td>1.567194</td>\n",
       "      <td>0.705320</td>\n",
       "      <td>0.703333</td>\n",
       "      <td>0.708969</td>\n",
       "      <td>0.703333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.035100</td>\n",
       "      <td>1.931715</td>\n",
       "      <td>0.692823</td>\n",
       "      <td>0.694667</td>\n",
       "      <td>0.698245</td>\n",
       "      <td>0.694667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.024400</td>\n",
       "      <td>2.107970</td>\n",
       "      <td>0.702400</td>\n",
       "      <td>0.703333</td>\n",
       "      <td>0.704906</td>\n",
       "      <td>0.703333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.027500</td>\n",
       "      <td>2.187114</td>\n",
       "      <td>0.709642</td>\n",
       "      <td>0.708667</td>\n",
       "      <td>0.712206</td>\n",
       "      <td>0.708667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>2.328472</td>\n",
       "      <td>0.711709</td>\n",
       "      <td>0.710667</td>\n",
       "      <td>0.713476</td>\n",
       "      <td>0.710667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.022200</td>\n",
       "      <td>2.401102</td>\n",
       "      <td>0.705762</td>\n",
       "      <td>0.705000</td>\n",
       "      <td>0.709364</td>\n",
       "      <td>0.705000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>2.387521</td>\n",
       "      <td>0.706714</td>\n",
       "      <td>0.706333</td>\n",
       "      <td>0.707705</td>\n",
       "      <td>0.706333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-09 19:40:49,999] Trial 3 finished with value: 2.8270856237578794 and parameters: {'learning_rate': 8.88119249739111e-05, 'num_train_epochs': 12, 'per_device_train_batch_size': 32, 'weight_decay': 0.01}. Best is trial 1 with value: 2.8897898528963553.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12000' max='12000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12000/12000 55:45, Epoch 16/16]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.742300</td>\n",
       "      <td>0.845425</td>\n",
       "      <td>0.682803</td>\n",
       "      <td>0.683667</td>\n",
       "      <td>0.698442</td>\n",
       "      <td>0.683667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.619200</td>\n",
       "      <td>0.796475</td>\n",
       "      <td>0.710523</td>\n",
       "      <td>0.714333</td>\n",
       "      <td>0.719615</td>\n",
       "      <td>0.714333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.514400</td>\n",
       "      <td>0.837782</td>\n",
       "      <td>0.702798</td>\n",
       "      <td>0.703000</td>\n",
       "      <td>0.714667</td>\n",
       "      <td>0.703000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.503900</td>\n",
       "      <td>0.882426</td>\n",
       "      <td>0.713708</td>\n",
       "      <td>0.713000</td>\n",
       "      <td>0.718596</td>\n",
       "      <td>0.713000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.218300</td>\n",
       "      <td>1.050960</td>\n",
       "      <td>0.701801</td>\n",
       "      <td>0.705333</td>\n",
       "      <td>0.705106</td>\n",
       "      <td>0.705333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.213900</td>\n",
       "      <td>1.187419</td>\n",
       "      <td>0.713595</td>\n",
       "      <td>0.713000</td>\n",
       "      <td>0.714479</td>\n",
       "      <td>0.713000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.113800</td>\n",
       "      <td>1.450071</td>\n",
       "      <td>0.702067</td>\n",
       "      <td>0.705000</td>\n",
       "      <td>0.701807</td>\n",
       "      <td>0.705000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.053400</td>\n",
       "      <td>1.664846</td>\n",
       "      <td>0.705569</td>\n",
       "      <td>0.705667</td>\n",
       "      <td>0.707251</td>\n",
       "      <td>0.705667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.094400</td>\n",
       "      <td>1.885956</td>\n",
       "      <td>0.698433</td>\n",
       "      <td>0.702333</td>\n",
       "      <td>0.701184</td>\n",
       "      <td>0.702333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.041700</td>\n",
       "      <td>1.983880</td>\n",
       "      <td>0.704501</td>\n",
       "      <td>0.704000</td>\n",
       "      <td>0.706822</td>\n",
       "      <td>0.704000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.012300</td>\n",
       "      <td>2.040560</td>\n",
       "      <td>0.705219</td>\n",
       "      <td>0.706333</td>\n",
       "      <td>0.705777</td>\n",
       "      <td>0.706333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>2.165524</td>\n",
       "      <td>0.702732</td>\n",
       "      <td>0.705000</td>\n",
       "      <td>0.703479</td>\n",
       "      <td>0.705000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>2.255440</td>\n",
       "      <td>0.707606</td>\n",
       "      <td>0.709000</td>\n",
       "      <td>0.708020</td>\n",
       "      <td>0.709000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>2.289299</td>\n",
       "      <td>0.709844</td>\n",
       "      <td>0.710667</td>\n",
       "      <td>0.710018</td>\n",
       "      <td>0.710667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>2.318399</td>\n",
       "      <td>0.713522</td>\n",
       "      <td>0.714000</td>\n",
       "      <td>0.713595</td>\n",
       "      <td>0.714000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>2.327927</td>\n",
       "      <td>0.710479</td>\n",
       "      <td>0.711333</td>\n",
       "      <td>0.710368</td>\n",
       "      <td>0.711333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-09 20:36:38,002] Trial 4 finished with value: 2.8435140166070676 and parameters: {'learning_rate': 1.5225886096750911e-05, 'num_train_epochs': 16, 'per_device_train_batch_size': 16, 'weight_decay': 0.01}. Best is trial 1 with value: 2.8897898528963553.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4500' max='4500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4500/4500 39:09, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.829800</td>\n",
       "      <td>0.828356</td>\n",
       "      <td>0.682681</td>\n",
       "      <td>0.683000</td>\n",
       "      <td>0.698302</td>\n",
       "      <td>0.683000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.647300</td>\n",
       "      <td>0.767328</td>\n",
       "      <td>0.717631</td>\n",
       "      <td>0.721000</td>\n",
       "      <td>0.724252</td>\n",
       "      <td>0.721000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.430300</td>\n",
       "      <td>0.850942</td>\n",
       "      <td>0.714860</td>\n",
       "      <td>0.714667</td>\n",
       "      <td>0.719345</td>\n",
       "      <td>0.714667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.349000</td>\n",
       "      <td>1.018722</td>\n",
       "      <td>0.709454</td>\n",
       "      <td>0.708000</td>\n",
       "      <td>0.712054</td>\n",
       "      <td>0.708000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.103800</td>\n",
       "      <td>1.309208</td>\n",
       "      <td>0.706096</td>\n",
       "      <td>0.707667</td>\n",
       "      <td>0.708548</td>\n",
       "      <td>0.707667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.108800</td>\n",
       "      <td>1.578663</td>\n",
       "      <td>0.708554</td>\n",
       "      <td>0.709000</td>\n",
       "      <td>0.712445</td>\n",
       "      <td>0.709000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.092600</td>\n",
       "      <td>1.800342</td>\n",
       "      <td>0.708159</td>\n",
       "      <td>0.710000</td>\n",
       "      <td>0.715637</td>\n",
       "      <td>0.710000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.026000</td>\n",
       "      <td>1.981465</td>\n",
       "      <td>0.714495</td>\n",
       "      <td>0.714000</td>\n",
       "      <td>0.716517</td>\n",
       "      <td>0.714000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.009200</td>\n",
       "      <td>1.990120</td>\n",
       "      <td>0.720728</td>\n",
       "      <td>0.722333</td>\n",
       "      <td>0.720674</td>\n",
       "      <td>0.722333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>2.101735</td>\n",
       "      <td>0.719911</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.720186</td>\n",
       "      <td>0.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>2.145178</td>\n",
       "      <td>0.717108</td>\n",
       "      <td>0.717333</td>\n",
       "      <td>0.717950</td>\n",
       "      <td>0.717333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>2.166435</td>\n",
       "      <td>0.718716</td>\n",
       "      <td>0.719000</td>\n",
       "      <td>0.719526</td>\n",
       "      <td>0.719000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-09 21:15:49,995] Trial 5 finished with value: 2.8762420095743537 and parameters: {'learning_rate': 6.189713308884082e-05, 'num_train_epochs': 12, 'per_device_train_batch_size': 32, 'weight_decay': 0.0}. Best is trial 1 with value: 2.8897898528963553.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='4500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 375/4500 03:11 < 35:22, 1.94 it/s, Epoch 1/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.859300</td>\n",
       "      <td>0.869185</td>\n",
       "      <td>0.675013</td>\n",
       "      <td>0.676000</td>\n",
       "      <td>0.694378</td>\n",
       "      <td>0.676000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-09 21:19:03,445] Trial 6 pruned. \n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='6000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 375/6000 03:12 < 48:20, 1.94 it/s, Epoch 1/16]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.876400</td>\n",
       "      <td>0.881128</td>\n",
       "      <td>0.675760</td>\n",
       "      <td>0.676667</td>\n",
       "      <td>0.694389</td>\n",
       "      <td>0.676667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-09 21:22:17,251] Trial 7 pruned. \n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='3750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 750/3750 03:25 < 13:43, 3.64 it/s, Epoch 1/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.039300</td>\n",
       "      <td>1.089916</td>\n",
       "      <td>0.611973</td>\n",
       "      <td>0.620333</td>\n",
       "      <td>0.622170</td>\n",
       "      <td>0.620333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-09 21:25:43,795] Trial 8 pruned. \n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='9000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 750/9000 03:25 < 37:44, 3.64 it/s, Epoch 1/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.761100</td>\n",
       "      <td>0.860003</td>\n",
       "      <td>0.683309</td>\n",
       "      <td>0.685000</td>\n",
       "      <td>0.694872</td>\n",
       "      <td>0.685000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-09 21:29:10,310] Trial 9 pruned. \n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Hyperparameter-Suche abgeschlossen ---\n",
      "Beste Trial:\n",
      "  Wert (F1): BestRun(run_id='1', objective=2.8897898528963553, hyperparameters={'learning_rate': 9.16873665647161e-05, 'num_train_epochs': 5, 'per_device_train_batch_size': 32, 'weight_decay': 0.01}, run_summary=None)\n",
      "\n",
      "------------------------------------------\n",
      "learning_rate: 9.16873665647161e-05\n",
      "num_train_epochs: 5\n",
      "per_device_train_batch_size : 32\n",
      "weight_decay: 0.01\n",
      "\n",
      "------------------END---------------------\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training the final model with the best hyperparameters...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1875' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1875/1875 17:09, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.816474</td>\n",
       "      <td>0.693687</td>\n",
       "      <td>0.694400</td>\n",
       "      <td>0.704766</td>\n",
       "      <td>0.694400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.878500</td>\n",
       "      <td>0.787844</td>\n",
       "      <td>0.712314</td>\n",
       "      <td>0.713800</td>\n",
       "      <td>0.717538</td>\n",
       "      <td>0.713800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.503600</td>\n",
       "      <td>0.897657</td>\n",
       "      <td>0.711471</td>\n",
       "      <td>0.712200</td>\n",
       "      <td>0.713717</td>\n",
       "      <td>0.712200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.236300</td>\n",
       "      <td>1.130081</td>\n",
       "      <td>0.708220</td>\n",
       "      <td>0.710200</td>\n",
       "      <td>0.708413</td>\n",
       "      <td>0.710200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.236300</td>\n",
       "      <td>1.372384</td>\n",
       "      <td>0.708856</td>\n",
       "      <td>0.708000</td>\n",
       "      <td>0.710061</td>\n",
       "      <td>0.708000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating the final model on the test set...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Set Evaluation Results:\n",
      "  eval_loss: 0.7832\n",
      "  eval_f1: 0.7199\n",
      "  eval_accuracy: 0.7220\n",
      "  eval_precision: 0.7227\n",
      "  eval_recall: 0.7220\n",
      "  eval_runtime: 15.3072\n",
      "  eval_samples_per_second: 195.9870\n",
      "  eval_steps_per_second: 24.4980\n",
      "  epoch: 5.0000\n",
      "\n",
      "Evaluating the final model on the validation set...\n",
      "\n",
      "Val Set Evaluation Results:\n",
      "  eval_loss: 0.7878\n",
      "  eval_f1: 0.7123\n",
      "  eval_accuracy: 0.7138\n",
      "  eval_precision: 0.7175\n",
      "  eval_recall: 0.7138\n",
      "  eval_runtime: 26.4359\n",
      "  eval_samples_per_second: 189.1370\n",
      "  eval_steps_per_second: 23.6420\n",
      "  epoch: 5.0000\n",
      "\n",
      " Saving the fine-tuned model and tokenizer...\n",
      "\n",
      "create predictions, to save in a df...\n",
      "--- 1. DF rdy ---\n",
      "Erstelle einen DataFrame nur mit den Vorhersagen, die vom Original abweichen (Validierungsset)...\n",
      "\n",
      "---  DFs rdy !!! ---\n",
      "--------------------------------------\n",
      "\n",
      "Anzahl der unterschiedlichen Vorhersagen im Validierungsset: 1431\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      " validation-set metrics (calculated from dfResults_pred):\n",
      "  Gewichteter F1-Score: 0.7123\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "Gesammelte Ergebnisse der Hyperparameter-Suche:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "runtimet: 264.21893575191496 min\n",
      "\n",
      " Script finished successfully!\n"
     ]
    }
   ],
   "source": [
    "boolHP = True # True: Hyperparameter-Suche, False: direktes Training mit festen Werten\n",
    "\n",
    "import time\n",
    "start_zeit = time.time()\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandarallel import pandarallel\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_recall_fscore_support\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer,  TrainingArguments, AutoConfig, AutoModelForSequenceClassification,DataCollatorWithPadding, TrainerCallback,TrainerState, TrainerControl\n",
    "import torch\n",
    "import os\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Initialisiere pandarallel für parallele Verarbeitung\n",
    "pandarallel.initialize(progress_bar=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Definieren Sie das Cache-Verzeichnis\n",
    "cache_dir = '/media/ubuntu/5d2d9f9d-a02d-45ab-865f-3d789a0c70f0/download/'\n",
    "os.environ['TRANSFORMERS_CACHE'] = cache_dir\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Dataset Klasse definieren\n",
    "class PublicationsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "class HPSearchResultLoggerCallback(TrainerCallback):\n",
    "    def on_evaluate(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, metrics: dict, **kwargs):\n",
    "        \"\"\"\n",
    "        Wird nach jeder Evaluation aufgerufen, auch während der HP-Suche für jeden Trial.\n",
    "        \"\"\"\n",
    "        if state.is_hyper_param_search:\n",
    "            # state.trial_params enthält die Hyperparameter des aktuellen Trials,\n",
    "            # wenn der HP-Such-Backend (z.B. Optuna) sie bereitstellt.\n",
    "            # Dies ist normalerweise der Fall.\n",
    "            current_hyperparameters = state.trial_params if state.trial_params is not None else {}\n",
    "\n",
    "            log_entry = {}\n",
    "            # Füge alle Hyperparameter hinzu\n",
    "            log_entry.update(current_hyperparameters)\n",
    "\n",
    "            # Füge die gewünschten Metriken hinzu\n",
    "            log_entry['eval_dataset_type'] = 'train/HP'\n",
    "            log_entry['eval_loss'] = metrics.get(\"eval_loss\")\n",
    "            log_entry['eval_accuracy'] = metrics.get(\"eval_accuracy\")\n",
    "            log_entry['eval_f1'] = metrics.get(\"eval_f1\")\n",
    "            log_entry['eval_precision'] = metrics.get(\"precision\")\n",
    "            log_entry['eval_recall'] = metrics.get(\"eval_recall\")\n",
    "            # Du kannst hier weitere Metriken hinzufügen, die von deiner compute_metrics Funktion zurückgegeben werden\n",
    "            # log_entry['eval_precision'] = metrics.get(\"eval_precision\")\n",
    "\n",
    "            # Überprüfe, ob bereits ein Eintrag mit exakt denselben Hyperparametern und Metriken vorhanden ist,\n",
    "            # um Duplikate zu vermeiden, falls on_evaluate mehrfach pro Trial aufgerufen wird (unwahrscheinlich, aber sicher ist sicher)\n",
    "            # In der Praxis wird on_evaluate normalerweise einmal pro Trial-Evaluation aufgerufen.\n",
    "            hp_search_results_list.append(log_entry)\n",
    "            print(\"test:\\n \")\n",
    "            print(current_hyperparameters)\n",
    "            print(\"test end\\n \")\n",
    "            #print(f\"HP Search Trial Logged: {log_entry}\") # Optional: zum Debuggen\n",
    "\n",
    "def clean_text(text):\n",
    "    # HTML-Tags entfernen\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    text = re.sub(r\"[\\\",\\']\",\"\", text)  #  Anführungszeichen entfernen\n",
    "\n",
    "    # 1. Mehrfache Anführungszeichen durch ein normales ' ersetzen\n",
    "    text = re.sub(r\"'{2,}\", \"'\", text)\n",
    "\n",
    "    # 2. HTML-Tags entfernen [1, 2, 3]\n",
    "    # Sucht nach Mustern wie <tag>Inhalt</tag> und ersetzt sie durch einen leeren String.\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "\n",
    "    # 3. URLs entfernen [1, 2, 3]\n",
    "    # Sucht nach gängigen URL-Mustern (http/https, www.) und ersetzt sie durch einen leeren String.\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "    # 4. E-Mail-IDs entfernen [3]\n",
    "    # Sucht nach E-Mail-Mustern (Zeichenfolge@Zeichenfolge.Domain) und ersetzt sie durch einen leeren String.\n",
    "    text = re.sub(r'\\S*@\\S*\\s?', '', text)\n",
    "\n",
    "    # 5. Zusätzliche Leerzeichen normalisieren [1, 4]\n",
    "    # Teilt den Text nach Leerzeichen auf und fügt ihn mit einem einzigen Leerzeichen wieder zusammen.\n",
    "    text = \" \".join(text.split())\n",
    "\n",
    "    text = re.sub(r\"[\\[,\\]]\",\"\", text)  # Mehrfache Leerzeichen zu einem reduzieren\n",
    "    \n",
    "\n",
    "    return text\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    preds = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    # Berechnung des gewichteten F1-Scores\n",
    "    f1 = f1_score(labels, preds, average='weighted')\n",
    "    \n",
    "    # Optional: Berechnung weiterer Metriken\n",
    "    precision, recall, _, _ = precision_recall_fscore_support(labels, preds, average='weighted', zero_division=0) # zero_division=0, um Warnungen zu vermeiden\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    \n",
    "    return {\n",
    "        'f1': f1,\n",
    "        'accuracy': acc,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "def model_init(trial):\n",
    "    # Laden Sie die Konfiguration zuerst, um sie an LoRaBertForSequenceClassification zu übergeben\n",
    "    # num_labels muss global oder als Argument verfügbar sein\n",
    "    model_name = 'distilbert/distilbert-base-multilingual-cased'\n",
    "    config = AutoConfig.from_pretrained(model_name, num_labels=num_labels, cache_dir=cache_dir)\n",
    "    \n",
    "\n",
    "    return AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        config=config,\n",
    "        cache_dir=cache_dir\n",
    "    )\n",
    "\n",
    "def time_now():\n",
    "    # Zeit funktion für den Dateinamen\n",
    "    current_dateTime = datetime.now()\n",
    "    time = str(current_dateTime.hour+2)+\"-\"+str(current_dateTime.minute)+\"_\"+str(current_dateTime.day) +\"-\"+ str(current_dateTime.month)+\"-\"+str(current_dateTime.year)\n",
    "    return str(time)\n",
    "\n",
    "def hp_space_optuna(trial):\n",
    "    # Hyperparameter-Suchraum für Optuna\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 1e-4, log=True),\n",
    "        \"num_train_epochs\": trial.suggest_categorical(\"num_train_epochs\",  [5, 12, 16]),\n",
    "        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [16, 32]),\n",
    "        \n",
    "        \"weight_decay\": trial.suggest_categorical(\"weight_decay\",  [0.0, 0.01]),\n",
    "    }\n",
    "        #\"warmup_ratio\": trial.suggest_float(\"warmup_ratio\", 0.0, 0.2),\n",
    "        \n",
    "        # Fügen Sie hier bei Bedarf weitere Hyperparameter hinzu\n",
    "        # \"lora_rank\": trial.suggest_categorical(\"lora_rank\", [2, 4, 8, 16]), # Wenn LoRa-Rank auch gesucht werden soll \n",
    "\n",
    "def prepare_val(df):\n",
    "    # Kombiniere Titel und Abstract\n",
    "    df['text'] = df['title'].astype(str) + \" - \" + df['abstract'].astype(str)\n",
    "\n",
    "    # Bereinigen Sie den Text\n",
    "    df[\"text\"] = df[\"text\"].parallel_apply(clean_text).str.lower()\n",
    "    # encode the labels\n",
    "    df['label_encoded'] = le.fit_transform(df['class']).astype(int)\n",
    "\n",
    "    df = df.sample(frac=1)\n",
    "    X_val = df[\"text\"]\n",
    "    Y_val = df[\"label_encoded\"]  \n",
    "\n",
    "    print(\"Tokenizing validation data...\")\n",
    "    val_encodings = tokenizer(\n",
    "        list(X_val), truncation=True, padding=True, max_length=512)\n",
    "    print(\"End...\")\n",
    "    print(\"creating dataset...\")\n",
    "    ## Dataset erstellen\n",
    "    val_dataset = PublicationsDataset(val_encodings, Y_val.reset_index(drop=True))\n",
    "    print(\"End...\")\n",
    "    return val_dataset,df\n",
    "\n",
    "def prepare_test_train(df):\n",
    "    # Kombiniere Titel und Abstract\n",
    "    df['text'] = df['title'].astype(str) + \" - \" + df['abstract'].astype(str)\n",
    "\n",
    "    # Bereinigen Sie den Text\n",
    "    df[\"text\"] = df[\"text\"].parallel_apply(clean_text).str.lower()\n",
    "\n",
    "    # encode the labels\n",
    "    df['label_encoded'] = le.fit_transform(df['class']).astype(int)\n",
    "\n",
    "    # train_test_split für das Training/Validation-Set (aus dfBert)\n",
    "    # Beachten Sie, dass X_test, y_test hier nur für das Training verwendet werden.\n",
    "    # dfBert_eval wird als separates Validierungsset für die Valedierung genutzt.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df['text'], df['label_encoded'], test_size=0.2, random_state=42, stratify=df['label_encoded'])\n",
    "\n",
    "    print(\"Tokenizing training data...\")\n",
    "    train_encodings = tokenizer(\n",
    "        list(X_train), truncation=True, padding=True, max_length=512)\n",
    "    test_encodings = tokenizer(\n",
    "        list(X_test), truncation=True, padding=True, max_length=512)\n",
    "    print(\"End...\")\n",
    "    print(\"creating dataset...\")\n",
    "    ## Dataset erstellen\n",
    "    train_dataset = PublicationsDataset(train_encodings, y_train.reset_index(drop=True))\n",
    "    test_dataset = PublicationsDataset(test_encodings, y_test.reset_index(drop=True))\n",
    "    print(\"End...\")\n",
    "    return train_dataset,test_dataset,df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- 1. Initialisierung ---\n",
    "\n",
    "hp_search_results_list = []\n",
    "final_run_results_list = []\n",
    "model_name = 'distilbert/distilbert-base-multilingual-cased'\n",
    "\n",
    "# # Definieren Sie den Pfad zu den Daten\n",
    "#path_train='../01_Daten/pkl/df_all_15k-1.pkl'\n",
    "path_train='../01_Daten/pkl/df_all_15k-2.pkl'\n",
    "#path_train='../01_Daten/pkl/df_all_15k-3.pkl'\n",
    "path_val='../01_Daten/pkl/df_val_5k-2.pkl'\n",
    "#path_val='../01_Daten/pkl/df_val_5k-3.pkl'\n",
    "\n",
    "\n",
    "#speicher Pfad für Logs und Modelle\n",
    "time_log_save = time_now()\n",
    "model_base_path = f\"../01_Daten/logs/{time_log_save}/{model_name} /\"\n",
    "model_log_path = model_base_path+\"logs/\"\n",
    "model_output_path = model_base_path+\"results/\"\n",
    "model_final_path = model_base_path+\"final_model/\"\n",
    "\n",
    "# LabelEncoder, tokenizer und  Data collator initialisieren\n",
    "le = LabelEncoder()\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name, cache_dir=cache_dir)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# --- 2. Erstellen HP Trainer und args---\n",
    "train_dataset, test_dataset, dfBert_train = prepare_test_train(pd.read_pickle(path_train))\n",
    "val_dataset, dfBert_val = prepare_val(pd.read_pickle(path_val)) # Das ist jetzt dein dediziertes Validierungsset\n",
    "\n",
    "if boolHP:\n",
    "\n",
    "    # num_labels auslesen für die Model-Initialisierung\n",
    "    num_labels = dfBert_train['label_encoded'].nunique()\n",
    "\n",
    "    # Trainingsparameter für die Hyperparameter-Suche, diese Werte dienen als Standardwerte oder Fallbacks.\n",
    "    # Die Werte aus Optuna (über hp_space_optuna) werden während der Trials verwendet.\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f'{model_output_path}results_hp_search', \n",
    "        \n",
    "        learning_rate=1e-5,\n",
    "        num_train_epochs=1, \n",
    "        per_device_train_batch_size= 16,        \n",
    "        \n",
    "        # Feste Werte für die Suche:\n",
    "        logging_dir=f'{model_log_path}logs_hp_search',\n",
    "        logging_steps=10,\n",
    "        report_to=\"tensorboard\",\n",
    "        eval_strategy=\"epoch\", \n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=1,\n",
    "        load_best_model_at_end=False,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        greater_is_better=True,\n",
    "    )\n",
    "    # Trainer initialisieren (ohne ein festes Modell - model_init wird verwendet)\n",
    "    trainer = Trainer(\n",
    "        model_init=model_init,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    # --- 3. Starten der Hyperparameter-Suche ---\n",
    "    print(\"Starte Hyperparameter-Suche...\")\n",
    "    # Starten der Hyperparameter-Suche\n",
    "    best_trial = trainer.hyperparameter_search(\n",
    "        direction=\"maximize\", # Maximiere den F1-Score\n",
    "        backend=\"optuna\",\n",
    "        n_trials=10, # Anzahl der Trials, die Optuna durchführen soll, je mehr Trials, desto länger dauert es, aber potenziell bessere Ergebnisse.\n",
    "        hp_space=hp_space_optuna,\n",
    "    )\n",
    "    print(\"\\n--- Hyperparameter-Suche abgeschlossen ---\")\n",
    "\n",
    "    print(\"Beste Trial:\")\n",
    "    print(f\"  Wert (F1): {best_trial}\") \n",
    "    print(\"\\n------------------------------------------\")\n",
    "    print(\"learning_rate: \"+str(best_trial.hyperparameters[\"learning_rate\"]))\n",
    "    print(\"num_train_epochs: \"+str(best_trial.hyperparameters[\"num_train_epochs\"]))\n",
    "    print(\"per_device_train_batch_size : \"+str(best_trial.hyperparameters[\"per_device_train_batch_size\"]))\n",
    "    #print(\"warmup_ratio: \"+str(best_trial.hyperparameters[\"warmup_ratio\"]))\n",
    "    print(\"weight_decay: \"+str(best_trial.hyperparameters[\"weight_decay\"]))\n",
    "    print(\"\\n------------------END---------------------\\n\\n\\n\")\n",
    "\n",
    "    # --- 4. Train with Best Hyperparameters ---\n",
    "    #update der TrainingArguments mit den besten Hyperparametern\n",
    "    best_hp = best_trial.hyperparameters\n",
    "\n",
    "\n",
    "    final_training_args = TrainingArguments(\n",
    "        output_dir=f\"{model_output_path}best_run\", \n",
    "        logging_dir=f\"{model_log_path}best_run\",\n",
    "        report_to=\"tensorboard\",\n",
    "        learning_rate=best_hp[\"learning_rate\"],\n",
    "        num_train_epochs=best_hp[\"num_train_epochs\"],\n",
    "        per_device_train_batch_size=best_hp[\"per_device_train_batch_size\"],\n",
    "        per_device_eval_batch_size=training_args.per_device_eval_batch_size, \n",
    "        weight_decay=best_hp.get(\"weight_decay\", training_args.weight_decay),\n",
    "        #warmup_ratio=best_hp.get(\"warmup_ratio\", training_args.warmup_ratio),\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        greater_is_better=True,\n",
    "        seed=42\n",
    "    )\n",
    "else:\n",
    "    final_training_args = TrainingArguments(\n",
    "    output_dir=f\"{model_output_path}best_run\", \n",
    "    logging_dir=f\"{model_log_path}best_run\",\n",
    "\n",
    "    learning_rate=2.7166361333742085e-05,\n",
    "    num_train_epochs= 5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=64,\n",
    "    weight_decay = 0.0,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit = 1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    seed=42\n",
    "    )\n",
    "\n",
    "# Initialisiere den Trainer mit den statischen Hyperparametern\n",
    "final_trainer = Trainer(\n",
    "    model_init=model_init,\n",
    "    args=final_training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"\\nTraining the final model with the best hyperparameters...\")\n",
    "final_trainer.train()\n",
    "\n",
    "# --- 5. Evaluate the Final Model ---\n",
    "print(\"\\nEvaluating the final model on the test set...\")\n",
    "test_results = final_trainer.evaluate(test_dataset)\n",
    "print(\"\\nTest Set Evaluation Results:\")\n",
    "for key, value in test_results.items():\n",
    "    print(f\"  {key}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nEvaluating the final model on the validation set...\")\n",
    "val_results = final_trainer.evaluate(val_dataset)\n",
    "print(\"\\nVal Set Evaluation Results:\")\n",
    "for key, value in val_results.items():\n",
    "    print(f\"  {key}: {value:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if boolHP:\n",
    "    # # --- 6. Save the Final Model & Tokenizer ---\n",
    "    end_zeit = time.time()\n",
    "    laufzeit = end_zeit - start_zeit\n",
    "    print(\"\\n Saving the fine-tuned model and tokenizer...\")\n",
    "    final_trainer.save_model(f\"{model_final_path}model\")\n",
    "    tokenizer.save_pretrained(f\"{model_final_path}tokenizer/\")\n",
    "\n",
    "\n",
    "\n",
    "# # --- 7. Predict and to DF ---\n",
    "# --- Erstellen eines DataFrames mit Vorhersagen für das Validierungsset ---\n",
    "print(\"\\ncreate predictions, to save in a df...\")\n",
    "\n",
    "predictions_output_val = final_trainer.predict(val_dataset)\n",
    "predicted_scores_val = predictions_output_val.predictions\n",
    "predicted_labels_encoded_val = np.argmax(predicted_scores_val, axis=1)\n",
    "predicted_labels_named_val = le.inverse_transform(predicted_labels_encoded_val)\n",
    "\n",
    "dfResults_pred = pd.DataFrame()\n",
    "\n",
    "dfResults_pred['id_im_aktuellen_df'] = dfBert_val.index.values\n",
    "\n",
    "# Ursprüngliche Klasse (Text-Label) aus dfBert_val\n",
    "dfResults_pred['class_original'] = dfBert_val['class'].values\n",
    "\n",
    "# Ursprüngliche Klasse (numerisch kodiertes Label - Ground Truth) aus dfBert_val\n",
    "dfResults_pred['label_encoded_original'] = dfBert_val['label_encoded'].values\n",
    "\n",
    "# Vorhergesagte Klasse (numerisch kodiertes Label)\n",
    "dfResults_pred['prediction_encoded'] = predicted_labels_encoded_val\n",
    "\n",
    "# Vorhergesagte Klasse (Text-Label)\n",
    "dfResults_pred['prediction_named'] = predicted_labels_named_val\n",
    "\n",
    "# Optional: Fügen Sie den Text hinzu, der für die Vorhersage verwendet wurde\n",
    "dfResults_pred['text_input'] = dfBert_val['text'].values\n",
    "\n",
    "\n",
    "print(\"--- 1. DF rdy ---\")\n",
    "print(\"Erstelle einen DataFrame nur mit den Vorhersagen, die vom Original abweichen (Validierungsset)...\")\n",
    "# Filtere dfResults_pred, um nur Zeilen zu erhalten, bei denen das Original-Label und das vorhergesagte Label unterschiedlich sind.\n",
    "dfResults_pred_diff = dfResults_pred[dfResults_pred['label_encoded_original'] != dfResults_pred['prediction_encoded']]\n",
    "print(\"\\n---  DFs rdy !!! ---\")\n",
    "if dfResults_pred_diff.empty:\n",
    "    print(\"Keine Unterschiede zwischen Original- und Vorhersage-Labels im Validierungsset gefunden. Perfekte Vorhersage!\")\n",
    "else:\n",
    "    print(\"--------------------------------------\")\n",
    "    print(f\"\\nAnzahl der unterschiedlichen Vorhersagen im Validierungsset: {len(dfResults_pred_diff)}\")\n",
    "print(\"\\n-------------------------------------------\")\n",
    "\n",
    "# Wahre Labels und vorhergesagte Labels aus dem DataFrame extrahieren\n",
    "y_true_val = dfResults_pred['label_encoded_original']\n",
    "y_pred_val = dfResults_pred['prediction_encoded']\n",
    "\n",
    "print(\"\\n validation-set metrics (calculated from dfResults_pred):\")\n",
    "\n",
    "#Gewichteter F1-Score\n",
    "f1_val_weighted = f1_score(y_true_val, y_pred_val, average='weighted', zero_division=0)\n",
    "print(f\"  Gewichteter F1-Score: {f1_val_weighted:.4f}\")\n",
    "print(\"\\n------------------------------------------\")\n",
    "\n",
    "\n",
    "dfResults_pred.to_pickle(f\"{model_base_path}dfResults_pred.pkl\")\n",
    "dfResults_pred_diff.to_pickle(f\"{model_base_path}dfResults_pred_diff({len(dfResults_pred_diff)}).pkl\")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"runtimet: {laufzeit/60} min\")\n",
    "print(\"\\n Script finished successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6776c191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BestRun(run_id='1', objective=2.8897898528963553, hyperparameters={'learning_rate': 9.16873665647161e-05, 'num_train_epochs': 5, 'per_device_train_batch_size': 32, 'weight_decay': 0.01}, run_summary=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af5be001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "Beste Trial:\n",
      "\n",
      "learning_rate: 9.16873665647161e-05\n",
      "num_train_epochs: 5\n",
      "per_device_train_batch_size : 32\n",
      "weight_decay: 0.01\n",
      "\n",
      "------------------END---------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"------------------------------------------\")\n",
    "print(\"Beste Trial:\\n\")\n",
    "\n",
    "print(\"learning_rate: \"+str(best_trial.hyperparameters[\"learning_rate\"]))\n",
    "print(\"num_train_epochs: \"+str(best_trial.hyperparameters[\"num_train_epochs\"]))\n",
    "print(\"per_device_train_batch_size : \"+str(best_trial.hyperparameters[\"per_device_train_batch_size\"]))\n",
    "print(\"weight_decay: \"+str(best_trial.hyperparameters[\"weight_decay\"]))\n",
    "#print(\"test: \"+str(best_trial.hyperparameters[\"warmup_ratio\"]))\n",
    "\n",
    "print(\"\\n------------------END---------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
