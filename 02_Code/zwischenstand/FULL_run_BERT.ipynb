{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df707d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 12 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b772a565fec44c6697028f6b35b534a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=1250), Label(value='0 / 1250'))), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing training data...\n",
      "End...\n",
      "creating dataset...\n",
      "End...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e7d36f182204fd0a10f2cdbd2a2dee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=417), Label(value='0 / 417'))), HB…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing validation data...\n",
      "End...\n",
      "creating dataset...\n",
      "End...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[I 2025-06-06 17:00:46,938] A new study created in memory with name: no-name-24906a9a-bd80-4863-a0b3-c63df342f47d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starte Hyperparameter-Suche...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3750' max='3750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3750/3750 32:47, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.694800</td>\n",
       "      <td>0.799700</td>\n",
       "      <td>0.707259</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.715138</td>\n",
       "      <td>0.708333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.559100</td>\n",
       "      <td>0.763570</td>\n",
       "      <td>0.720241</td>\n",
       "      <td>0.722333</td>\n",
       "      <td>0.721989</td>\n",
       "      <td>0.722333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.482700</td>\n",
       "      <td>0.782440</td>\n",
       "      <td>0.729301</td>\n",
       "      <td>0.729333</td>\n",
       "      <td>0.732521</td>\n",
       "      <td>0.729333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.436200</td>\n",
       "      <td>0.838012</td>\n",
       "      <td>0.724378</td>\n",
       "      <td>0.725667</td>\n",
       "      <td>0.723749</td>\n",
       "      <td>0.725667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.251900</td>\n",
       "      <td>0.875079</td>\n",
       "      <td>0.722807</td>\n",
       "      <td>0.725333</td>\n",
       "      <td>0.723395</td>\n",
       "      <td>0.725333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-06 17:33:37,612] Trial 0 finished with value: 2.8968689041637288 and parameters: {'learning_rate': 1.3887966021119244e-05, 'num_train_epochs': 5, 'per_device_train_batch_size': 16, 'weight_decay': 0.01}. Best is trial 0 with value: 2.8968689041637288.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9000' max='9000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9000/9000 1:18:34, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.705900</td>\n",
       "      <td>0.841747</td>\n",
       "      <td>0.693634</td>\n",
       "      <td>0.695667</td>\n",
       "      <td>0.704523</td>\n",
       "      <td>0.695667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.647400</td>\n",
       "      <td>0.772593</td>\n",
       "      <td>0.716810</td>\n",
       "      <td>0.718333</td>\n",
       "      <td>0.716999</td>\n",
       "      <td>0.718333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.601500</td>\n",
       "      <td>0.769596</td>\n",
       "      <td>0.719510</td>\n",
       "      <td>0.720667</td>\n",
       "      <td>0.722377</td>\n",
       "      <td>0.720667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.606300</td>\n",
       "      <td>0.777698</td>\n",
       "      <td>0.721183</td>\n",
       "      <td>0.722333</td>\n",
       "      <td>0.722077</td>\n",
       "      <td>0.722333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.416700</td>\n",
       "      <td>0.857599</td>\n",
       "      <td>0.714554</td>\n",
       "      <td>0.718333</td>\n",
       "      <td>0.724932</td>\n",
       "      <td>0.718333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.462300</td>\n",
       "      <td>0.832496</td>\n",
       "      <td>0.724601</td>\n",
       "      <td>0.724667</td>\n",
       "      <td>0.727559</td>\n",
       "      <td>0.724667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.284300</td>\n",
       "      <td>0.887910</td>\n",
       "      <td>0.718438</td>\n",
       "      <td>0.722333</td>\n",
       "      <td>0.721833</td>\n",
       "      <td>0.722333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.215100</td>\n",
       "      <td>0.960523</td>\n",
       "      <td>0.717830</td>\n",
       "      <td>0.721667</td>\n",
       "      <td>0.720806</td>\n",
       "      <td>0.721667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.340900</td>\n",
       "      <td>1.020036</td>\n",
       "      <td>0.709991</td>\n",
       "      <td>0.714000</td>\n",
       "      <td>0.717264</td>\n",
       "      <td>0.714000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.252700</td>\n",
       "      <td>1.034502</td>\n",
       "      <td>0.709405</td>\n",
       "      <td>0.714667</td>\n",
       "      <td>0.713417</td>\n",
       "      <td>0.714667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.260800</td>\n",
       "      <td>1.067336</td>\n",
       "      <td>0.708459</td>\n",
       "      <td>0.711667</td>\n",
       "      <td>0.711799</td>\n",
       "      <td>0.711667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.218300</td>\n",
       "      <td>1.065811</td>\n",
       "      <td>0.712086</td>\n",
       "      <td>0.715000</td>\n",
       "      <td>0.713213</td>\n",
       "      <td>0.715000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-06 18:52:15,352] Trial 1 finished with value: 2.8552989071578216 and parameters: {'learning_rate': 5.945723536491367e-06, 'num_train_epochs': 12, 'per_device_train_batch_size': 16, 'weight_decay': 0.01}. Best is trial 0 with value: 2.8968689041637288.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9000' max='9000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9000/9000 1:18:27, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.680600</td>\n",
       "      <td>0.797576</td>\n",
       "      <td>0.714734</td>\n",
       "      <td>0.716000</td>\n",
       "      <td>0.723030</td>\n",
       "      <td>0.716000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.502300</td>\n",
       "      <td>0.786794</td>\n",
       "      <td>0.718664</td>\n",
       "      <td>0.721333</td>\n",
       "      <td>0.730383</td>\n",
       "      <td>0.721333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.346400</td>\n",
       "      <td>0.924183</td>\n",
       "      <td>0.709927</td>\n",
       "      <td>0.712333</td>\n",
       "      <td>0.717887</td>\n",
       "      <td>0.712333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.429500</td>\n",
       "      <td>1.284651</td>\n",
       "      <td>0.711572</td>\n",
       "      <td>0.715333</td>\n",
       "      <td>0.714408</td>\n",
       "      <td>0.715333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.152000</td>\n",
       "      <td>1.706107</td>\n",
       "      <td>0.694096</td>\n",
       "      <td>0.696667</td>\n",
       "      <td>0.697318</td>\n",
       "      <td>0.696667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.098400</td>\n",
       "      <td>1.848857</td>\n",
       "      <td>0.712916</td>\n",
       "      <td>0.712333</td>\n",
       "      <td>0.714222</td>\n",
       "      <td>0.712333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.041300</td>\n",
       "      <td>2.081402</td>\n",
       "      <td>0.707747</td>\n",
       "      <td>0.707333</td>\n",
       "      <td>0.716202</td>\n",
       "      <td>0.707333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.042700</td>\n",
       "      <td>2.122175</td>\n",
       "      <td>0.711876</td>\n",
       "      <td>0.713667</td>\n",
       "      <td>0.711563</td>\n",
       "      <td>0.713667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>2.215179</td>\n",
       "      <td>0.712673</td>\n",
       "      <td>0.713000</td>\n",
       "      <td>0.712524</td>\n",
       "      <td>0.713000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>2.335338</td>\n",
       "      <td>0.714159</td>\n",
       "      <td>0.714000</td>\n",
       "      <td>0.715356</td>\n",
       "      <td>0.714000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>2.366576</td>\n",
       "      <td>0.714202</td>\n",
       "      <td>0.714333</td>\n",
       "      <td>0.716293</td>\n",
       "      <td>0.714333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.015500</td>\n",
       "      <td>2.401434</td>\n",
       "      <td>0.715812</td>\n",
       "      <td>0.716667</td>\n",
       "      <td>0.716367</td>\n",
       "      <td>0.716667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-06 20:10:46,467] Trial 2 finished with value: 2.8655125060462963 and parameters: {'learning_rate': 4.274882008055365e-05, 'num_train_epochs': 12, 'per_device_train_batch_size': 16, 'weight_decay': 0.0}. Best is trial 0 with value: 2.8968689041637288.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12000' max='12000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12000/12000 1:44:54, Epoch 16/16]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.689700</td>\n",
       "      <td>0.815242</td>\n",
       "      <td>0.704007</td>\n",
       "      <td>0.704667</td>\n",
       "      <td>0.714006</td>\n",
       "      <td>0.704667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.524500</td>\n",
       "      <td>0.781247</td>\n",
       "      <td>0.717090</td>\n",
       "      <td>0.721000</td>\n",
       "      <td>0.723548</td>\n",
       "      <td>0.721000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.408100</td>\n",
       "      <td>0.858584</td>\n",
       "      <td>0.717323</td>\n",
       "      <td>0.717667</td>\n",
       "      <td>0.727578</td>\n",
       "      <td>0.717667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.408700</td>\n",
       "      <td>1.014542</td>\n",
       "      <td>0.715662</td>\n",
       "      <td>0.717667</td>\n",
       "      <td>0.719931</td>\n",
       "      <td>0.717667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.211800</td>\n",
       "      <td>1.387007</td>\n",
       "      <td>0.717138</td>\n",
       "      <td>0.718333</td>\n",
       "      <td>0.717061</td>\n",
       "      <td>0.718333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.278000</td>\n",
       "      <td>1.563900</td>\n",
       "      <td>0.712334</td>\n",
       "      <td>0.711000</td>\n",
       "      <td>0.715146</td>\n",
       "      <td>0.711000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.114000</td>\n",
       "      <td>1.912544</td>\n",
       "      <td>0.706031</td>\n",
       "      <td>0.706667</td>\n",
       "      <td>0.712598</td>\n",
       "      <td>0.706667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.022900</td>\n",
       "      <td>1.919653</td>\n",
       "      <td>0.715287</td>\n",
       "      <td>0.716667</td>\n",
       "      <td>0.716187</td>\n",
       "      <td>0.716667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.102000</td>\n",
       "      <td>2.094717</td>\n",
       "      <td>0.714188</td>\n",
       "      <td>0.714333</td>\n",
       "      <td>0.717062</td>\n",
       "      <td>0.714333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.038400</td>\n",
       "      <td>2.249606</td>\n",
       "      <td>0.709098</td>\n",
       "      <td>0.711000</td>\n",
       "      <td>0.709844</td>\n",
       "      <td>0.711000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>2.493133</td>\n",
       "      <td>0.703173</td>\n",
       "      <td>0.702333</td>\n",
       "      <td>0.714791</td>\n",
       "      <td>0.702333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>2.430381</td>\n",
       "      <td>0.711685</td>\n",
       "      <td>0.713667</td>\n",
       "      <td>0.714538</td>\n",
       "      <td>0.713667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.036900</td>\n",
       "      <td>2.503408</td>\n",
       "      <td>0.706151</td>\n",
       "      <td>0.708000</td>\n",
       "      <td>0.709025</td>\n",
       "      <td>0.708000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>2.469222</td>\n",
       "      <td>0.715178</td>\n",
       "      <td>0.718000</td>\n",
       "      <td>0.716149</td>\n",
       "      <td>0.718000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.004900</td>\n",
       "      <td>2.561969</td>\n",
       "      <td>0.710581</td>\n",
       "      <td>0.711333</td>\n",
       "      <td>0.713439</td>\n",
       "      <td>0.711333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>2.527356</td>\n",
       "      <td>0.711225</td>\n",
       "      <td>0.712333</td>\n",
       "      <td>0.711323</td>\n",
       "      <td>0.712333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-06 21:55:44,192] Trial 3 finished with value: 2.8472149963091913 and parameters: {'learning_rate': 2.4703090985463156e-05, 'num_train_epochs': 16, 'per_device_train_batch_size': 16, 'weight_decay': 0.01}. Best is trial 0 with value: 2.8968689041637288.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12000' max='12000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12000/12000 1:44:47, Epoch 16/16]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.678200</td>\n",
       "      <td>0.788499</td>\n",
       "      <td>0.712358</td>\n",
       "      <td>0.714667</td>\n",
       "      <td>0.717980</td>\n",
       "      <td>0.714667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.474800</td>\n",
       "      <td>0.838535</td>\n",
       "      <td>0.713056</td>\n",
       "      <td>0.716333</td>\n",
       "      <td>0.726391</td>\n",
       "      <td>0.716333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.371200</td>\n",
       "      <td>1.005361</td>\n",
       "      <td>0.699659</td>\n",
       "      <td>0.703000</td>\n",
       "      <td>0.711402</td>\n",
       "      <td>0.703000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.305100</td>\n",
       "      <td>1.306467</td>\n",
       "      <td>0.716749</td>\n",
       "      <td>0.718667</td>\n",
       "      <td>0.716630</td>\n",
       "      <td>0.718667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.192700</td>\n",
       "      <td>1.603266</td>\n",
       "      <td>0.707245</td>\n",
       "      <td>0.709000</td>\n",
       "      <td>0.713082</td>\n",
       "      <td>0.709000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.113700</td>\n",
       "      <td>1.919270</td>\n",
       "      <td>0.703081</td>\n",
       "      <td>0.704333</td>\n",
       "      <td>0.705258</td>\n",
       "      <td>0.704333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.086400</td>\n",
       "      <td>1.994917</td>\n",
       "      <td>0.721919</td>\n",
       "      <td>0.722000</td>\n",
       "      <td>0.722779</td>\n",
       "      <td>0.722000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.015600</td>\n",
       "      <td>2.289403</td>\n",
       "      <td>0.713798</td>\n",
       "      <td>0.714333</td>\n",
       "      <td>0.717612</td>\n",
       "      <td>0.714333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.051200</td>\n",
       "      <td>2.441628</td>\n",
       "      <td>0.704965</td>\n",
       "      <td>0.707000</td>\n",
       "      <td>0.713765</td>\n",
       "      <td>0.707000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.036900</td>\n",
       "      <td>2.546951</td>\n",
       "      <td>0.716056</td>\n",
       "      <td>0.713000</td>\n",
       "      <td>0.726626</td>\n",
       "      <td>0.713000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>2.660055</td>\n",
       "      <td>0.715136</td>\n",
       "      <td>0.713667</td>\n",
       "      <td>0.723216</td>\n",
       "      <td>0.713667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>2.693888</td>\n",
       "      <td>0.715658</td>\n",
       "      <td>0.718000</td>\n",
       "      <td>0.715639</td>\n",
       "      <td>0.718000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>2.760460</td>\n",
       "      <td>0.712371</td>\n",
       "      <td>0.713333</td>\n",
       "      <td>0.713213</td>\n",
       "      <td>0.713333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.791179</td>\n",
       "      <td>0.715951</td>\n",
       "      <td>0.716000</td>\n",
       "      <td>0.717684</td>\n",
       "      <td>0.716000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.836895</td>\n",
       "      <td>0.718554</td>\n",
       "      <td>0.719667</td>\n",
       "      <td>0.718506</td>\n",
       "      <td>0.719667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>2.848938</td>\n",
       "      <td>0.713235</td>\n",
       "      <td>0.714000</td>\n",
       "      <td>0.714708</td>\n",
       "      <td>0.714000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-06 23:40:34,611] Trial 4 finished with value: 2.8559430439455022 and parameters: {'learning_rate': 6.323079170820639e-05, 'num_train_epochs': 16, 'per_device_train_batch_size': 16, 'weight_decay': 0.01}. Best is trial 0 with value: 2.8968689041637288.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='9000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 750/9000 06:30 < 1:11:44, 1.92 it/s, Epoch 1/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.759500</td>\n",
       "      <td>0.869399</td>\n",
       "      <td>0.680665</td>\n",
       "      <td>0.683333</td>\n",
       "      <td>0.687490</td>\n",
       "      <td>0.683333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-06 23:47:06,449] Trial 5 pruned. \n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='9000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 750/9000 06:29 < 1:11:40, 1.92 it/s, Epoch 1/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.914400</td>\n",
       "      <td>0.971885</td>\n",
       "      <td>0.655170</td>\n",
       "      <td>0.659667</td>\n",
       "      <td>0.662449</td>\n",
       "      <td>0.659667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-06 23:53:37,864] Trial 6 pruned. \n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='3750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 750/3750 06:29 < 26:04, 1.92 it/s, Epoch 1/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.700900</td>\n",
       "      <td>0.834177</td>\n",
       "      <td>0.697052</td>\n",
       "      <td>0.698667</td>\n",
       "      <td>0.707881</td>\n",
       "      <td>0.698667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 00:00:09,443] Trial 7 pruned. \n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6000' max='6000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6000/6000 1:40:14, Epoch 16/16]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.780100</td>\n",
       "      <td>0.837083</td>\n",
       "      <td>0.694973</td>\n",
       "      <td>0.697333</td>\n",
       "      <td>0.706152</td>\n",
       "      <td>0.697333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.710200</td>\n",
       "      <td>0.764775</td>\n",
       "      <td>0.724975</td>\n",
       "      <td>0.724000</td>\n",
       "      <td>0.728033</td>\n",
       "      <td>0.724000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.647400</td>\n",
       "      <td>0.761649</td>\n",
       "      <td>0.722097</td>\n",
       "      <td>0.723333</td>\n",
       "      <td>0.724235</td>\n",
       "      <td>0.723333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.532100</td>\n",
       "      <td>0.780747</td>\n",
       "      <td>0.722120</td>\n",
       "      <td>0.723667</td>\n",
       "      <td>0.721573</td>\n",
       "      <td>0.723667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.377500</td>\n",
       "      <td>0.842413</td>\n",
       "      <td>0.712563</td>\n",
       "      <td>0.715667</td>\n",
       "      <td>0.717186</td>\n",
       "      <td>0.715667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.413700</td>\n",
       "      <td>0.867122</td>\n",
       "      <td>0.719196</td>\n",
       "      <td>0.716667</td>\n",
       "      <td>0.727474</td>\n",
       "      <td>0.716667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>0.940965</td>\n",
       "      <td>0.717154</td>\n",
       "      <td>0.719333</td>\n",
       "      <td>0.717163</td>\n",
       "      <td>0.719333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.185700</td>\n",
       "      <td>1.047325</td>\n",
       "      <td>0.697967</td>\n",
       "      <td>0.700667</td>\n",
       "      <td>0.703211</td>\n",
       "      <td>0.700667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.136100</td>\n",
       "      <td>1.138389</td>\n",
       "      <td>0.700706</td>\n",
       "      <td>0.705667</td>\n",
       "      <td>0.704945</td>\n",
       "      <td>0.705667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.077700</td>\n",
       "      <td>1.193070</td>\n",
       "      <td>0.697927</td>\n",
       "      <td>0.702000</td>\n",
       "      <td>0.700109</td>\n",
       "      <td>0.702000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.070400</td>\n",
       "      <td>1.278668</td>\n",
       "      <td>0.702909</td>\n",
       "      <td>0.704667</td>\n",
       "      <td>0.705842</td>\n",
       "      <td>0.704667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.082900</td>\n",
       "      <td>1.347342</td>\n",
       "      <td>0.707087</td>\n",
       "      <td>0.711000</td>\n",
       "      <td>0.709049</td>\n",
       "      <td>0.711000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.077200</td>\n",
       "      <td>1.437036</td>\n",
       "      <td>0.704594</td>\n",
       "      <td>0.706667</td>\n",
       "      <td>0.705813</td>\n",
       "      <td>0.706667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.051000</td>\n",
       "      <td>1.484134</td>\n",
       "      <td>0.707800</td>\n",
       "      <td>0.710667</td>\n",
       "      <td>0.709057</td>\n",
       "      <td>0.710667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.032200</td>\n",
       "      <td>1.498032</td>\n",
       "      <td>0.710341</td>\n",
       "      <td>0.712000</td>\n",
       "      <td>0.710384</td>\n",
       "      <td>0.712000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.009700</td>\n",
       "      <td>1.517047</td>\n",
       "      <td>0.705530</td>\n",
       "      <td>0.708000</td>\n",
       "      <td>0.705964</td>\n",
       "      <td>0.708000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 01:40:27,480] Trial 8 finished with value: 2.8274937808228913 and parameters: {'learning_rate': 1.000075293533579e-05, 'num_train_epochs': 16, 'per_device_train_batch_size': 32, 'weight_decay': 0.0}. Best is trial 0 with value: 2.8968689041637288.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='12000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  750/12000 06:29 < 1:37:42, 1.92 it/s, Epoch 1/16]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.872500</td>\n",
       "      <td>0.946692</td>\n",
       "      <td>0.660015</td>\n",
       "      <td>0.664333</td>\n",
       "      <td>0.667860</td>\n",
       "      <td>0.664333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 01:46:59,024] Trial 9 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Hyperparameter-Suche abgeschlossen ---\n",
      "Beste Trial:\n",
      "  Wert (F1): BestRun(run_id='0', objective=2.8968689041637288, hyperparameters={'learning_rate': 1.3887966021119244e-05, 'num_train_epochs': 5, 'per_device_train_batch_size': 16, 'weight_decay': 0.01}, run_summary=None)\n",
      "\n",
      "------------------------------------------\n",
      "learning_rate: 1.3887966021119244e-05\n",
      "num_train_epochs: 5\n",
      "per_device_train_batch_size : 16\n",
      "weight_decay: 0.01\n",
      "\n",
      "------------------END---------------------\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training the final model with the best hyperparameters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3750' max='3750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3750/3750 34:20, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.009900</td>\n",
       "      <td>0.789270</td>\n",
       "      <td>0.703541</td>\n",
       "      <td>0.704600</td>\n",
       "      <td>0.709980</td>\n",
       "      <td>0.704600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.672000</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.720619</td>\n",
       "      <td>0.722600</td>\n",
       "      <td>0.726562</td>\n",
       "      <td>0.722600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.527400</td>\n",
       "      <td>0.788171</td>\n",
       "      <td>0.726256</td>\n",
       "      <td>0.726400</td>\n",
       "      <td>0.729091</td>\n",
       "      <td>0.726400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.400400</td>\n",
       "      <td>0.850398</td>\n",
       "      <td>0.719721</td>\n",
       "      <td>0.721400</td>\n",
       "      <td>0.720317</td>\n",
       "      <td>0.721400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.333700</td>\n",
       "      <td>0.884342</td>\n",
       "      <td>0.719478</td>\n",
       "      <td>0.722200</td>\n",
       "      <td>0.722397</td>\n",
       "      <td>0.722200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating the final model on the test set...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Set Evaluation Results:\n",
      "  eval_loss: 0.7820\n",
      "  eval_f1: 0.7307\n",
      "  eval_accuracy: 0.7307\n",
      "  eval_precision: 0.7337\n",
      "  eval_recall: 0.7307\n",
      "  eval_runtime: 29.5144\n",
      "  eval_samples_per_second: 101.6450\n",
      "  eval_steps_per_second: 12.7060\n",
      "  epoch: 5.0000\n",
      "\n",
      "Evaluating the final model on the validation set...\n",
      "\n",
      "Val Set Evaluation Results:\n",
      "  eval_loss: 0.7882\n",
      "  eval_f1: 0.7263\n",
      "  eval_accuracy: 0.7264\n",
      "  eval_precision: 0.7291\n",
      "  eval_recall: 0.7264\n",
      "  eval_runtime: 50.1399\n",
      "  eval_samples_per_second: 99.7210\n",
      "  eval_steps_per_second: 12.4650\n",
      "  epoch: 5.0000\n",
      "\n",
      " Saving the fine-tuned model and tokenizer...\n",
      "\n",
      "create predictions, to save in a df...\n",
      "--- 1. DF rdy ---\n",
      "Erstelle einen DataFrame nur mit den Vorhersagen, die vom Original abweichen (Validierungsset)...\n",
      "\n",
      "---  DFs rdy !!! ---\n",
      "--------------------------------------\n",
      "\n",
      "Anzahl der unterschiedlichen Vorhersagen im Validierungsset: 1368\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      " validation-set metrics (calculated from dfResults_pred):\n",
      "  Gewichteter F1-Score: 0.7263\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "Gesammelte Ergebnisse der Hyperparameter-Suche:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "runtimet: 563.4244112332661 min\n",
      "\n",
      " Script finished successfully!\n"
     ]
    }
   ],
   "source": [
    "boolHP = True # True: Hyperparameter-Suche, False: direktes Training mit festen Werten\n",
    "\n",
    "import time\n",
    "start_zeit = time.time()\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandarallel import pandarallel\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_recall_fscore_support\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer,  TrainingArguments, AutoConfig, AutoModelForSequenceClassification,DataCollatorWithPadding, TrainerCallback,TrainerState, TrainerControl\n",
    "import torch\n",
    "import os\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Initialisiere pandarallel für parallele Verarbeitung\n",
    "pandarallel.initialize(progress_bar=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Definieren Sie das Cache-Verzeichnis\n",
    "cache_dir = '/media/ubuntu/5d2d9f9d-a02d-45ab-865f-3d789a0c70f0/download/'\n",
    "os.environ['TRANSFORMERS_CACHE'] = cache_dir\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Dataset Klasse definieren\n",
    "class PublicationsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "class HPSearchResultLoggerCallback(TrainerCallback):\n",
    "    def on_evaluate(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, metrics: dict, **kwargs):\n",
    "        \"\"\"\n",
    "        Wird nach jeder Evaluation aufgerufen, auch während der HP-Suche für jeden Trial.\n",
    "        \"\"\"\n",
    "        if state.is_hyper_param_search:\n",
    "            # state.trial_params enthält die Hyperparameter des aktuellen Trials,\n",
    "            # wenn der HP-Such-Backend (z.B. Optuna) sie bereitstellt.\n",
    "            # Dies ist normalerweise der Fall.\n",
    "            current_hyperparameters = state.trial_params if state.trial_params is not None else {}\n",
    "\n",
    "            log_entry = {}\n",
    "            # Füge alle Hyperparameter hinzu\n",
    "            log_entry.update(current_hyperparameters)\n",
    "\n",
    "            # Füge die gewünschten Metriken hinzu\n",
    "            log_entry['eval_dataset_type'] = 'train/HP'\n",
    "            log_entry['eval_loss'] = metrics.get(\"eval_loss\")\n",
    "            log_entry['eval_accuracy'] = metrics.get(\"eval_accuracy\")\n",
    "            log_entry['eval_f1'] = metrics.get(\"eval_f1\")\n",
    "            log_entry['eval_precision'] = metrics.get(\"precision\")\n",
    "            log_entry['eval_recall'] = metrics.get(\"eval_recall\")\n",
    "            # Du kannst hier weitere Metriken hinzufügen, die von deiner compute_metrics Funktion zurückgegeben werden\n",
    "            # log_entry['eval_precision'] = metrics.get(\"eval_precision\")\n",
    "\n",
    "            # Überprüfe, ob bereits ein Eintrag mit exakt denselben Hyperparametern und Metriken vorhanden ist,\n",
    "            # um Duplikate zu vermeiden, falls on_evaluate mehrfach pro Trial aufgerufen wird (unwahrscheinlich, aber sicher ist sicher)\n",
    "            # In der Praxis wird on_evaluate normalerweise einmal pro Trial-Evaluation aufgerufen.\n",
    "            hp_search_results_list.append(log_entry)\n",
    "            print(\"test:\\n \")\n",
    "            print(current_hyperparameters)\n",
    "            print(\"test end\\n \")\n",
    "            #print(f\"HP Search Trial Logged: {log_entry}\") # Optional: zum Debuggen\n",
    "\n",
    "def clean_text(text):\n",
    "    # HTML-Tags entfernen\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    text = re.sub(r\"[\\\",\\']\",\"\", text)  #  Anführungszeichen entfernen\n",
    "\n",
    "    # 1. Mehrfache Anführungszeichen durch ein normales ' ersetzen\n",
    "    text = re.sub(r\"'{2,}\", \"'\", text)\n",
    "\n",
    "    # 2. HTML-Tags entfernen [1, 2, 3]\n",
    "    # Sucht nach Mustern wie <tag>Inhalt</tag> und ersetzt sie durch einen leeren String.\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "\n",
    "    # 3. URLs entfernen [1, 2, 3]\n",
    "    # Sucht nach gängigen URL-Mustern (http/https, www.) und ersetzt sie durch einen leeren String.\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "    # 4. E-Mail-IDs entfernen [3]\n",
    "    # Sucht nach E-Mail-Mustern (Zeichenfolge@Zeichenfolge.Domain) und ersetzt sie durch einen leeren String.\n",
    "    text = re.sub(r'\\S*@\\S*\\s?', '', text)\n",
    "\n",
    "    # 5. Zusätzliche Leerzeichen normalisieren [1, 4]\n",
    "    # Teilt den Text nach Leerzeichen auf und fügt ihn mit einem einzigen Leerzeichen wieder zusammen.\n",
    "    text = \" \".join(text.split())\n",
    "\n",
    "    text = re.sub(r\"[\\[,\\]]\",\"\", text)  # Mehrfache Leerzeichen zu einem reduzieren\n",
    "    \n",
    "\n",
    "    return text\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    preds = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    # Berechnung des gewichteten F1-Scores\n",
    "    f1 = f1_score(labels, preds, average='weighted')\n",
    "    \n",
    "    # Optional: Berechnung weiterer Metriken\n",
    "    precision, recall, _, _ = precision_recall_fscore_support(labels, preds, average='weighted', zero_division=0) # zero_division=0, um Warnungen zu vermeiden\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    \n",
    "    return {\n",
    "        'f1': f1,\n",
    "        'accuracy': acc,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "def model_init(trial):\n",
    "    # Laden Sie die Konfiguration zuerst, um sie an LoRaBertForSequenceClassification zu übergeben\n",
    "    # num_labels muss global oder als Argument verfügbar sein\n",
    "    model_name = 'bert-base-uncased'\n",
    "    config = AutoConfig.from_pretrained(model_name, num_labels=num_labels, cache_dir=cache_dir)\n",
    "    \n",
    "\n",
    "    return BertForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        config=config,\n",
    "        cache_dir=cache_dir\n",
    "    )\n",
    "\n",
    "def time_now():\n",
    "    # Zeit funktion für den Dateinamen\n",
    "    current_dateTime = datetime.now()\n",
    "    time = str(current_dateTime.hour+2)+\"-\"+str(current_dateTime.minute)+\"_\"+str(current_dateTime.day) +\"-\"+ str(current_dateTime.month)+\"-\"+str(current_dateTime.year)\n",
    "    return str(time)\n",
    "\n",
    "def hp_space_optuna(trial):\n",
    "    # Hyperparameter-Suchraum für Optuna\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 1e-4, log=True),\n",
    "        \"num_train_epochs\": trial.suggest_categorical(\"num_train_epochs\",  [5, 12, 16]),\n",
    "        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [16, 32]),\n",
    "        \"weight_decay\": trial.suggest_categorical(\"weight_decay\",  [0.0, 0.01]),\n",
    "    }\n",
    "        \n",
    "\n",
    "def prepare_val(df):\n",
    "    # Kombiniere Titel und Abstract\n",
    "    df['text'] = df['title'].astype(str) + \" - \" + df['abstract'].astype(str)\n",
    "\n",
    "    # Bereinigen Sie den Text\n",
    "    df[\"text\"] = df[\"text\"].parallel_apply(clean_text).str.lower()\n",
    "    # encode the labels\n",
    "    df['label_encoded'] = le.fit_transform(df['class']).astype(int)\n",
    "\n",
    "    df = df.sample(frac=1)\n",
    "    X_val = df[\"text\"]\n",
    "    Y_val = df[\"label_encoded\"]  \n",
    "\n",
    "    print(\"Tokenizing validation data...\")\n",
    "    val_encodings = tokenizer(\n",
    "        list(X_val), truncation=True, padding=True, max_length=512)\n",
    "    print(\"End...\")\n",
    "    print(\"creating dataset...\")\n",
    "    ## Dataset erstellen\n",
    "    val_dataset = PublicationsDataset(val_encodings, Y_val.reset_index(drop=True))\n",
    "    print(\"End...\")\n",
    "    return val_dataset,df\n",
    "\n",
    "def prepare_test_train(df):\n",
    "    # Kombiniere Titel und Abstract\n",
    "    df['text'] = df['title'].astype(str) + \" - \" + df['abstract'].astype(str)\n",
    "\n",
    "    # Bereinigen Sie den Text\n",
    "    df[\"text\"] = df[\"text\"].parallel_apply(clean_text).str.lower()\n",
    "\n",
    "    # encode the labels\n",
    "    df['label_encoded'] = le.fit_transform(df['class']).astype(int)\n",
    "\n",
    "    # train_test_split für das Training/Validation-Set (aus dfBert)\n",
    "    # Beachten Sie, dass X_test, y_test hier nur für das Training verwendet werden.\n",
    "    # dfBert_eval wird als separates Validierungsset für die Valedierung genutzt.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df['text'], df['label_encoded'], test_size=0.2, random_state=42, stratify=df['label_encoded'])\n",
    "\n",
    "    print(\"Tokenizing training data...\")\n",
    "    train_encodings = tokenizer(\n",
    "        list(X_train), truncation=True, padding=True, max_length=512)\n",
    "    test_encodings = tokenizer(\n",
    "        list(X_test), truncation=True, padding=True, max_length=512)\n",
    "    print(\"End...\")\n",
    "    print(\"creating dataset...\")\n",
    "    ## Dataset erstellen\n",
    "    train_dataset = PublicationsDataset(train_encodings, y_train.reset_index(drop=True))\n",
    "    test_dataset = PublicationsDataset(test_encodings, y_test.reset_index(drop=True))\n",
    "    print(\"End...\")\n",
    "    return train_dataset,test_dataset,df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- 1. Initialisierung ---\n",
    "\n",
    "hp_search_results_list = []\n",
    "final_run_results_list = []\n",
    "model_name = 'bert-base-uncased'\n",
    "\n",
    "# # Definieren Sie den Pfad zu den Daten\n",
    "#path_train='../01_Daten/pkl/df_all_15k-1.pkl'\n",
    "path_train='../01_Daten/pkl/df_all_15k-2.pkl'\n",
    "#path_train='../01_Daten/pkl/df_all_15k-3.pkl'\n",
    "path_val='../01_Daten/pkl/df_val_5k-2.pkl'\n",
    "#path_val='../01_Daten/pkl/df_val_5k-3.pkl'\n",
    "\n",
    "\n",
    "#speicher Pfad für Logs und Modelle\n",
    "time_log_save = time_now()\n",
    "model_base_path = f\"../01_Daten/logs/{time_log_save}/bert_multiclass_FT-15k/\"\n",
    "model_log_path = model_base_path+\"logs/\"\n",
    "model_output_path = model_base_path+\"results/\"\n",
    "model_final_path = model_base_path+\"final_model/\"\n",
    "\n",
    "# LabelEncoder, tokenizer und  Data collator initialisieren\n",
    "le = LabelEncoder()\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name, cache_dir=cache_dir)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# --- 2. Erstellen HP Trainer und args---\n",
    "train_dataset, test_dataset, dfBert_train = prepare_test_train(pd.read_pickle(path_train))\n",
    "val_dataset, dfBert_val = prepare_val(pd.read_pickle(path_val)) # Das ist jetzt dein dediziertes Validierungsset\n",
    "\n",
    "# num_labels auslesen für die Model-Initialisierung\n",
    "num_labels = dfBert_train['label_encoded'].nunique()\n",
    "\n",
    "if boolHP:\n",
    "\n",
    "\n",
    "\n",
    "    # Trainingsparameter für die Hyperparameter-Suche, diese Werte dienen als Standardwerte oder Fallbacks.\n",
    "    # Die Werte aus Optuna (über hp_space_optuna) werden während der Trials verwendet.\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f'{model_output_path}results_hp_search', \n",
    "        \n",
    "        learning_rate=1e-5,\n",
    "        num_train_epochs=1, \n",
    "        per_device_train_batch_size= 16,        \n",
    "        \n",
    "        # Feste Werte für die Suche:\n",
    "        logging_dir=f'{model_log_path}logs_hp_search',\n",
    "        logging_steps=10,\n",
    "        report_to=\"tensorboard\",\n",
    "        eval_strategy=\"epoch\", \n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=1,\n",
    "        load_best_model_at_end=False,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        greater_is_better=True,\n",
    "    )\n",
    "    # Trainer initialisieren (ohne ein festes Modell - model_init wird verwendet)\n",
    "    trainer = Trainer(\n",
    "        model_init=model_init,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    # --- 3. Starten der Hyperparameter-Suche ---\n",
    "    print(\"Starte Hyperparameter-Suche...\")\n",
    "    # Starten der Hyperparameter-Suche\n",
    "    best_trial = trainer.hyperparameter_search(\n",
    "        direction=\"maximize\", # Maximiere den F1-Score\n",
    "        backend=\"optuna\",\n",
    "        n_trials=10, # Anzahl der Trials, die Optuna durchführen soll, je mehr Trials, desto länger dauert es, aber potenziell bessere Ergebnisse.\n",
    "        hp_space=hp_space_optuna,\n",
    "    )\n",
    "    print(\"\\n--- Hyperparameter-Suche abgeschlossen ---\")\n",
    "\n",
    "    print(\"Beste Trial:\")\n",
    "    print(f\"  Wert (F1): {best_trial}\") \n",
    "    print(\"\\n------------------------------------------\")\n",
    "    print(\"learning_rate: \"+str(best_trial.hyperparameters[\"learning_rate\"]))\n",
    "    print(\"num_train_epochs: \"+str(best_trial.hyperparameters[\"num_train_epochs\"]))\n",
    "    print(\"per_device_train_batch_size : \"+str(best_trial.hyperparameters[\"per_device_train_batch_size\"]))\n",
    "    #print(\"warmup_ratio: \"+str(best_trial.hyperparameters[\"warmup_ratio\"]))\n",
    "    print(\"weight_decay: \"+str(best_trial.hyperparameters[\"weight_decay\"]))\n",
    "    print(\"\\n------------------END---------------------\\n\\n\\n\")\n",
    "\n",
    "    # --- 4. Train with Best Hyperparameters ---\n",
    "    #update der TrainingArguments mit den besten Hyperparametern\n",
    "    best_hp = best_trial.hyperparameters\n",
    "\n",
    "\n",
    "    final_training_args = TrainingArguments(\n",
    "        output_dir=f\"{model_output_path}best_run\", \n",
    "        logging_dir=f\"{model_log_path}best_run\",\n",
    "        report_to=\"tensorboard\",\n",
    "        learning_rate=best_hp[\"learning_rate\"],\n",
    "        num_train_epochs=best_hp[\"num_train_epochs\"],\n",
    "        per_device_train_batch_size=best_hp[\"per_device_train_batch_size\"],\n",
    "        per_device_eval_batch_size=training_args.per_device_eval_batch_size, \n",
    "        weight_decay=best_hp.get(\"weight_decay\", training_args.weight_decay),\n",
    "        #warmup_ratio=best_hp.get(\"warmup_ratio\", training_args.warmup_ratio),\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        greater_is_better=True,\n",
    "        seed=42\n",
    "    )\n",
    "else:\n",
    "    final_training_args = TrainingArguments(\n",
    "    output_dir=f\"{model_output_path}best_run\", \n",
    "    logging_dir=f\"{model_log_path}best_run\",\n",
    "\n",
    "    learning_rate=2.7166361333742085e-05,\n",
    "    num_train_epochs= 5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=64,\n",
    "    weight_decay = 0.0,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit = 1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    seed=42\n",
    "    )\n",
    "\n",
    "# Initialisiere den Trainer mit den statischen Hyperparametern\n",
    "final_trainer = Trainer(\n",
    "    model_init=model_init,\n",
    "    args=final_training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"\\nTraining the final model with the best hyperparameters...\")\n",
    "final_trainer.train()\n",
    "\n",
    "# --- 5. Evaluate the Final Model ---\n",
    "print(\"\\nEvaluating the final model on the test set...\")\n",
    "test_results = final_trainer.evaluate(test_dataset)\n",
    "print(\"\\nTest Set Evaluation Results:\")\n",
    "for key, value in test_results.items():\n",
    "    print(f\"  {key}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nEvaluating the final model on the validation set...\")\n",
    "val_results = final_trainer.evaluate(val_dataset)\n",
    "print(\"\\nVal Set Evaluation Results:\")\n",
    "for key, value in val_results.items():\n",
    "    print(f\"  {key}: {value:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if boolHP:\n",
    "    # # --- 6. Save the Final Model & Tokenizer ---\n",
    "    end_zeit = time.time()\n",
    "    laufzeit = end_zeit - start_zeit\n",
    "    print(\"\\n Saving the fine-tuned model and tokenizer...\")\n",
    "    final_trainer.save_model(f\"{model_final_path}model\")\n",
    "    tokenizer.save_pretrained(f\"{model_final_path}tokenizer/\")\n",
    "\n",
    "\n",
    "\n",
    "# # --- 7. Predict and to DF ---\n",
    "# --- Erstellen eines DataFrames mit Vorhersagen für das Validierungsset ---\n",
    "print(\"\\ncreate predictions, to save in a df...\")\n",
    "\n",
    "predictions_output_val = final_trainer.predict(val_dataset)\n",
    "predicted_scores_val = predictions_output_val.predictions\n",
    "predicted_labels_encoded_val = np.argmax(predicted_scores_val, axis=1)\n",
    "predicted_labels_named_val = le.inverse_transform(predicted_labels_encoded_val)\n",
    "\n",
    "dfResults_pred = pd.DataFrame()\n",
    "\n",
    "dfResults_pred['id_im_aktuellen_df'] = dfBert_val.index.values\n",
    "\n",
    "# Ursprüngliche Klasse (Text-Label) aus dfBert_val\n",
    "dfResults_pred['class_original'] = dfBert_val['class'].values\n",
    "\n",
    "# Ursprüngliche Klasse (numerisch kodiertes Label - Ground Truth) aus dfBert_val\n",
    "dfResults_pred['label_encoded_original'] = dfBert_val['label_encoded'].values\n",
    "\n",
    "# Vorhergesagte Klasse (numerisch kodiertes Label)\n",
    "dfResults_pred['prediction_encoded'] = predicted_labels_encoded_val\n",
    "\n",
    "# Vorhergesagte Klasse (Text-Label)\n",
    "dfResults_pred['prediction_named'] = predicted_labels_named_val\n",
    "\n",
    "# Optional: Fügen Sie den Text hinzu, der für die Vorhersage verwendet wurde\n",
    "dfResults_pred['text_input'] = dfBert_val['text'].values\n",
    "\n",
    "\n",
    "print(\"--- 1. DF rdy ---\")\n",
    "print(\"Erstelle einen DataFrame nur mit den Vorhersagen, die vom Original abweichen (Validierungsset)...\")\n",
    "# Filtere dfResults_pred, um nur Zeilen zu erhalten, bei denen das Original-Label und das vorhergesagte Label unterschiedlich sind.\n",
    "dfResults_pred_diff = dfResults_pred[dfResults_pred['label_encoded_original'] != dfResults_pred['prediction_encoded']]\n",
    "print(\"\\n---  DFs rdy !!! ---\")\n",
    "if dfResults_pred_diff.empty:\n",
    "    print(\"Keine Unterschiede zwischen Original- und Vorhersage-Labels im Validierungsset gefunden. Perfekte Vorhersage!\")\n",
    "else:\n",
    "    print(\"--------------------------------------\")\n",
    "    print(f\"\\nAnzahl der unterschiedlichen Vorhersagen im Validierungsset: {len(dfResults_pred_diff)}\")\n",
    "print(\"\\n-------------------------------------------\")\n",
    "\n",
    "# Wahre Labels und vorhergesagte Labels aus dem DataFrame extrahieren\n",
    "y_true_val = dfResults_pred['label_encoded_original']\n",
    "y_pred_val = dfResults_pred['prediction_encoded']\n",
    "\n",
    "print(\"\\n validation-set metrics (calculated from dfResults_pred):\")\n",
    "\n",
    "#Gewichteter F1-Score\n",
    "f1_val_weighted = f1_score(y_true_val, y_pred_val, average='weighted', zero_division=0)\n",
    "print(f\"  Gewichteter F1-Score: {f1_val_weighted:.4f}\")\n",
    "print(\"\\n------------------------------------------\")\n",
    "\n",
    "\n",
    "dfResults_pred.to_pickle(f\"{model_base_path}dfResults_pred.pkl\")\n",
    "dfResults_pred_diff.to_pickle(f\"{model_base_path}dfResults_pred_diff({len(dfResults_pred_diff)}).pkl\")\n",
    "\n",
    "\n",
    "\n",
    "# Wandle die gesammelten Ergebnisse in einen DataFrame um\n",
    "df_hp_results = pd.DataFrame(hp_search_results_list)\n",
    "print(\"\\nGesammelte Ergebnisse der Hyperparameter-Suche:\")\n",
    "print(df_hp_results)\n",
    "\n",
    "print(f\"runtimet: {laufzeit/60} min\")\n",
    "print(\"\\n Script finished successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6776c191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BestRun(run_id='0', objective=2.8968689041637288, hyperparameters={'learning_rate': 1.3887966021119244e-05, 'num_train_epochs': 5, 'per_device_train_batch_size': 16, 'weight_decay': 0.01}, run_summary=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af5be001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "Beste Trial:\n",
      "\n",
      "learning_rate: 1.3887966021119244e-05\n",
      "num_train_epochs: 5\n",
      "per_device_train_batch_size : 16\n",
      "weight_decay: 0.01\n",
      "\n",
      "------------------END---------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"------------------------------------------\")\n",
    "print(\"Beste Trial:\\n\")\n",
    "\n",
    "print(\"learning_rate: \"+str(best_trial.hyperparameters[\"learning_rate\"]))\n",
    "print(\"num_train_epochs: \"+str(best_trial.hyperparameters[\"num_train_epochs\"]))\n",
    "print(\"per_device_train_batch_size : \"+str(best_trial.hyperparameters[\"per_device_train_batch_size\"]))\n",
    "print(\"weight_decay: \"+str(best_trial.hyperparameters[\"weight_decay\"]))\n",
    "#print(\"test: \"+str(best_trial.hyperparameters[\"warmup_ratio\"]))\n",
    "\n",
    "print(\"\\n------------------END---------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85fe595",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "459629c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feba8518",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path=\"../01_Daten/logs/final/bert_multiclass_FT-15k/final_model/\"\n",
    "path_val=\"../01_Daten/pkl/df_val_5k-3.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57dd29a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_val(df):\n",
    "    # Kombiniere Titel und Abstract\n",
    "    df['text'] = df['title'].astype(str) + \" - \" + df['abstract'].astype(str)\n",
    "\n",
    "    # Bereinigen Sie den Text\n",
    "    df[\"text\"] = df[\"text\"].parallel_apply(clean_text).str.lower()\n",
    "    # encode the labels\n",
    "    df['label_encoded'] = le.fit_transform(df['class']).astype(int)\n",
    "\n",
    "    df = df.sample(frac=1)\n",
    "    X_val = df[\"text\"]\n",
    "    Y_val = df[\"label_encoded\"]  \n",
    "\n",
    "    print(\"Tokenizing validation data...\")\n",
    "    val_encodings = tokenizer(\n",
    "        list(X_val), truncation=True, padding=True, max_length=512)\n",
    "    print(\"End...\")\n",
    "    print(\"creating dataset...\")\n",
    "    ## Dataset erstellen\n",
    "    val_dataset = PublicationsDataset(val_encodings, Y_val.reset_index(drop=True))\n",
    "    print(\"End...\")\n",
    "    return val_dataset,df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6465d234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "\n",
      "validation-set metrics (calculated from dfResults_pred):\n",
      "F1-Score: 0.7059\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "Anzahl der unterschiedlichen Vorhersagen im Validierungsset: 1462\n",
      "\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "loaded_model =model_path+\"model\"\n",
    "loaded_tokenizer=model_path+\"tokenizer\"\n",
    "classifier = pipeline(\"text-classification\", model=loaded_model, tokenizer=loaded_tokenizer, device=0 if torch.cuda.is_available() else -1)\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Load Validation DF\n",
    "dfBert_val = prepare_val(pd.read_pickle(path_val))\n",
    "\n",
    "# Klassifikation auf jede Zeile anwenden\n",
    "dfBert_val['predictions_encoded'] = dfBert_val['text'].apply(lambda x:  classifier(x, truncation=True, max_length=512)[0]['label'])\n",
    "dfBert_val['predictions_encoded'] = dfBert_val['predictions_encoded'].apply(lambda x: int(x[6:]))\n",
    "dfBert_val['predicted_labels_named'] = dfBert_val['predictions_encoded'].apply(lambda x: le.inverse_transform([x])[0])\n",
    "\n",
    "# Wahre Labels und vorhergesagte Labels aus dem DataFrame extrahieren\n",
    "y_true_val = dfBert_val['label_encoded']\n",
    "y_pred_val = dfBert_val['predictions_encoded']\n",
    "\n",
    "\n",
    "\n",
    "print(\"------------------------------------------\")\n",
    "print(\"\\nvalidation-set metrics (calculated from dfResults_pred):\")\n",
    "#F1-Score\n",
    "f1_val_weighted = f1_score(y_true_val, y_pred_val, average='weighted', zero_division=0)\n",
    "print(f\"F1-Score: {f1_val_weighted:.4f}\")\n",
    "print(\"\\n------------------------------------------\")\n",
    "\n",
    "\n",
    "dfResults_pred_diff = dfBert_val[dfBert_val['label_encoded'] != dfBert_val['predictions_encoded']]\n",
    "if dfResults_pred_diff.empty:\n",
    "    print(\"\\nKeine Unterschiede zwischen Original- und Vorhersage-Labels im Validierungsset gefunden. Perfekte Vorhersage!\")\n",
    "else:\n",
    "    print(f\"\\nAnzahl der unterschiedlichen Vorhersagen im Validierungsset: {len(dfResults_pred_diff)}\")\n",
    "print(\"\\n-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ba346f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4400d8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a74d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path checker\n",
    "import os\n",
    "                \n",
    "model_path = \"../01_Daten/logs/FULL_bert_multiclass_FT-15k/bert_multiclass_FT-15k/final_model/model\"\n",
    "if os.path.exists(model_path):\n",
    "    print(f\"Model directory exists at: {model_path}\")\n",
    "else:\n",
    "    print(f\"Model directory not found at: {model_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
